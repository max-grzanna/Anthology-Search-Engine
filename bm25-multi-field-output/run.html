<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="icon"
    href="https://raw.githubusercontent.com/capreolus-ir/diffir/master/docs/images/icon.png">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
  <title>diffir: IR model comparision</title>
  <style>
    .card {
      margin: 5px !important;
    }

    .highlight {
      background-color: #ffffd3;
    }

    #DocumentOverlay {
      position: fixed;
      top: 0;
      bottom: 0;
      left: 0;
      right: 0;
      background-color: rgba(0, 0, 0, .25);
    }

    #DocumentDetails {
      position: fixed;
      top: 60px;
      left: 10%;
      right: 10%;
      bottom: 60px;
      background-color: white;
      padding: 20px;
      border: 1px solid rgba(0, 0, 0, .125);
      border-radius: 0.25rem;
      box-shadow: 0 0 16px black;
      overflow: auto;
    }

    .close-overlay {
      position: absolute;
      top: 4px;
      right: 4px;
      border-radius: 100%;
      background-color: #111;
      font-size: 17px;
      padding: 9px;
      color: white;
      width: 30px;
      height: 30px;
      text-align: center;
      font-weight: normal;
      line-height: 11px;
      cursor: pointer;
    }

    .docid {
      background-color: rgb(224, 135, 55);
      position: absolute;
      top: 0;
      bottom: 0;
      width: 20px;
      overflow: hidden;
      margin-bottom: 0;
      border-radius: 0;
      font-weight: normal;
      white-space: nowrap;
    }

    .docid-value {
      transform: rotate(90deg);
      font-size: 0.7em;
      padding-left: 8px;
    }

    .fields th {
      vertical-align: top;
      text-align: right;
      padding-right: 12px;
      color: #999;
      font-weight: normal;
    }

    #query-container {
      max-width: 600px;
      border: 1px solid #999;
      border-radius: 0.25rem;
      margin: 20px auto;
      padding: 10px;
    }

    .other-rank {
      font-size: 1.2em;
      display: inline-block;
      width: 20px;
      margin-top: 46px;
      margin-left: 3px;
      margin-right: 3px;
      cursor: help;
    }

    mark {
      padding: 0;
      font-weight: bold;
    }

    .snippet {
      font-size: 0.9em;
      line-height: 1.2;
    }

    .elip {
      text-align: center;
      margin: 16px;
      color: gray;
    }

    .doc-info {
      white-space: nowrap;
    }

    .card-header {
      min-height: 128px;
      cursor: pointer;
    }

    .swatch {
      display: inline-block;
      width: 16px;
      height: 16px;
      vertical-align: middle;
    }

    .form-group {
      width: 150px;
      height: 20px;
      padding-left: 10px;
      padding-top: 10px;
    }

    .nobackground {
      background: transparent !important;
      font-weight: normal;
    }
    .styled-table {
      margin-left: 0px;
      margin-top: 10px;
      border-collapse: collapse;    
      font-size: 0.9em;
      /* font-family: sans-serif;       */
      min-width: 350px;      
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
    }
    .styled-table thead tr {
      background-color: #17a2b8;
      color: #ffffff;
      text-align: left;
    }    
    /* .styled-table th, */
    .styled-table td {
      /* padding: 12px 15px; */
      text-align: center;
    }    
    .styled-table tbody tr {
      color: #ffffff;
    }
    /*
    .styled-table tbody tr:nth-of-type(even) {
      background-color: #f3f3f3;
    } */

    .styled-table tbody tr:last-of-type {
      border-bottom: 2px solid #009879;
    }

    #ranking-summary ul li span {
      margin-right: 5px;
    }    
  </style>
</head>

<body>
  <header>
    <div class="collapse bg-dark" id="navbarHeader">
      <div class="container">
        <div class="row">
          <div class="col-sm-6 col-md-6 py-4">
            <h6 class="text-white">Summary</h6>
            <p class="text-white" id="ranking-summary"></p>
          </div>
          <div class="col-sm-5 col-md-5 py-4"> 
            <h6 class="text-white">Ranking statistics</h6>                                   
            <ul>
              <li class="text-white"><span id="contrast-measure"></span></li>
              <li class="text-white"> <span>Relevance metrics</span> <div id="metrics"></div></li>
            </ul>                        
          </div>
        </div>
      </div>
    </div>
    <div class="navbar navbar-dark bg-dark box-shadow navbar-fixed-top">
      <div class="container d-flex justify-content-between">
        <a href="#" class="navbar-brand d-flex align-items-center">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-search"
            viewBox="0 0 16 16">
            <path
              d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z" />
          </svg>
          <strong style="padding-left:  5px;">DiffIR</strong>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarHeader"
          aria-controls="navbarHeader" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
    </div>
  </header>
  <div class="container">
    <div class="input-group" style="padding-top: 50px; padding-bottom: 10px; background-color: white;">
      <select id="Queries" data-width="100%" data-style="border" data-container="body"></select>
    </div>
    <div id="query-container" class="sticky-top" style="background-color: white;">
      <div style="position:relative">
        <button id="query-collapse-btn" style="position: absolute; top: 12px; right: 8px;" type="button"
          class="btn btn-outline-info btn-sm" data-toggle="collapse" data-target=".query_collapse" aria-expanded="false"
          aria-controls="query_collapse">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
            class="bi bi-arrows-angle-expand" viewBox="0 0 16 16">
            <path fill-rule="evenodd"
              d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z" />
          </svg>
        </button>
      </div>
      <div id="Query" style="padding-right: 45px;">
      </div>
    </div>
    <div class="row justify-content-center" id="runName">
      <div class="col">
        <h6 id="Run1Name" style="text-align: center;"></h6>
      </div>
      <div class="col">
        <h6 id="Run2Name" style="text-align: center;"></h6>
      </div>
    </div>
    <div class="row" id="docList">
      <div id="Run1Docs" class="col">
      </div>
      <div id="Run2Docs" class="col">
      </div>
    </div>    
  </div>
  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
    integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
    integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>
  <script type="text/javascript">
    var data = {"meta": {"run1_name": "/tira-data/output/run.txt", "run2_name": null, "dataset": "iranthology-memory", "measure": "topk", "qrelDefs": {"0": "Not Relevant", "1": "Relevant"}, "queryFields": ["query_id", "title", "description", "narrative"], "docFields": ["doc_id", "abstract", "title", "authors", "year", "booktitle"], "relevanceColors": {"null": "#888888", "0": "#d54541", "1": "#52b262"}}, "queries": [{"fields": {"query_id": "1", "title": "Deep Neural Networks", "description": "Which papers talk about Deep Neural Networks?", "narrative": "Relevant papers have at least one mention of Deep Neural. Paper containing only Deep or Neural are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [0.8], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [0.866947989864271]}, "run_1": [{"doc_id": "2017.cikm_conference-2017.312", "score": 86.743205104434, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[15, 19, 1.0], [20, 26, 1.0], [27, 35, 1.0], [216, 220, 1.0], [221, 227, 1.0], [228, 236, 1.0], [426, 430, 1.0], [431, 437, 1.0], [438, 446, 1.0], [688, 692, 1.0], [693, 699, 1.0], [700, 708, 1.0], [755, 761, 1.0], [762, 770, 1.0], [772, 776, 1.0], [807, 813, 1.0], [930, 934, 1.0], [935, 941, 1.0], [942, 950, 1.0]]}, "snippet": {"field": "default_text", "start": 683, "stop": 883, "weights": [[5, 9, 1.0], [10, 16, 1.0], [17, 25, 1.0], [72, 78, 1.0], [79, 87, 1.0], [89, 93, 1.0], [124, 130, 1.0]]}}, {"doc_id": "2020.wsdm_conference-2020.4", "score": 85.75466577357159, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[4, 8, 1.0], [26, 32, 1.0], [33, 41, 1.0], [53, 57, 1.0], [107, 111, 1.0], [112, 118, 1.0], [119, 127, 1.0], [184, 188, 1.0], [213, 217, 1.0], [264, 270, 1.0], [271, 279, 1.0], [360, 364, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 8, 1.0], [26, 32, 1.0], [33, 41, 1.0], [53, 57, 1.0], [107, 111, 1.0], [112, 118, 1.0], [119, 127, 1.0], [184, 188, 1.0]]}}, {"doc_id": "2020.wsdm_conference-2020.126", "score": 84.38048028521881, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[21, 25, 1.0], [26, 32, 1.0], [33, 41, 1.0], [165, 169, 1.0], [170, 176, 1.0], [177, 185, 1.0], [269, 273, 1.0], [587, 591, 1.0], [592, 598, 1.0], [599, 607, 1.0], [844, 848, 1.0], [849, 855, 1.0], [856, 864, 1.0], [954, 958, 1.0], [959, 965, 1.0], [1162, 1166, 1.0], [1216, 1222, 1.0], [1223, 1231, 1.0]]}, "snippet": {"field": "default_text", "start": 16, "stop": 216, "weights": [[5, 9, 1.0], [10, 16, 1.0], [17, 25, 1.0], [149, 153, 1.0], [154, 160, 1.0], [161, 169, 1.0]]}}, {"doc_id": "2018.sigirconf_workshop-2018ecom.33", "score": 81.2285436428226, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[30, 34, 1.0], [35, 41, 1.0], [42, 50, 1.0], [78, 82, 1.0], [162, 166, 1.0], [167, 173, 1.0], [303, 307, 1.0], [308, 314, 1.0], [315, 323, 1.0], [459, 463, 1.0], [464, 470, 1.0], [471, 479, 1.0]]}, "snippet": {"field": "default_text", "start": 25, "stop": 225, "weights": [[5, 9, 1.0], [10, 16, 1.0], [17, 25, 1.0], [53, 57, 1.0], [137, 141, 1.0], [142, 148, 1.0]]}}, {"doc_id": "2017.cikm_conference-2017.271", "score": 81.0335584805895, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[0, 4, 1.0], [5, 11, 1.0], [12, 20, 1.0], [450, 454, 1.0], [455, 461, 1.0], [553, 559, 1.0], [728, 734, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[0, 4, 1.0], [5, 11, 1.0], [12, 20, 1.0]]}}, {"doc_id": "2018.wwwconf_conference-2018c.377", "score": 77.7005121989692, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[47, 51, 1.0], [52, 58, 1.0], [59, 67, 1.0], [278, 284, 1.0], [461, 467, 1.0], [529, 535, 1.0], [722, 726, 1.0], [727, 733, 1.0]]}, "snippet": {"field": "default_text", "start": 42, "stop": 242, "weights": [[5, 9, 1.0], [10, 16, 1.0], [17, 25, 1.0]]}}, {"doc_id": "2016.sigirconf_conference-2016.150", "score": 77.58451179594068, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[39, 43, 1.0], [44, 50, 1.0], [322, 326, 1.0], [380, 386, 1.0], [413, 419, 1.0], [809, 815, 1.0]]}, "snippet": {"field": "default_text", "start": 317, "stop": 517, "weights": [[5, 9, 1.0], [63, 69, 1.0], [96, 102, 1.0]]}}, {"doc_id": "2018.wwwconf_conference-2018.17", "score": 76.99463691362456, "relevance": null, "rank": 8, "weights": {"doc_id": [], "default_text": [[8, 12, 1.0], [13, 19, 1.0], [107, 111, 1.0], [112, 118, 1.0], [916, 920, 1.0], [921, 927, 1.0]]}, "snippet": {"field": "default_text", "start": 3, "stop": 203, "weights": [[5, 9, 1.0], [10, 16, 1.0], [104, 108, 1.0], [109, 115, 1.0]]}}, {"doc_id": "2017.cikm_conference-2017.151", "score": 76.64269003645, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[65, 69, 1.0], [70, 76, 1.0], [144, 152, 1.0], [212, 216, 1.0], [217, 223, 1.0], [283, 291, 1.0], [867, 871, 1.0], [915, 921, 1.0], [979, 983, 1.0], [1093, 1099, 1.0]]}, "snippet": {"field": "default_text", "start": 60, "stop": 260, "weights": [[5, 9, 1.0], [10, 16, 1.0], [84, 92, 1.0], [152, 156, 1.0], [157, 163, 1.0]]}}, {"doc_id": "2015.sigirconf_conference-2015.135", "score": 75.16249495845544, "relevance": null, "rank": 10, "weights": {"doc_id": [], "default_text": [[32, 36, 1.0], [51, 57, 1.0], [58, 66, 1.0], [100, 104, 1.0], [265, 271, 1.0], [416, 422, 1.0], [501, 505, 1.0]]}, "snippet": {"field": "default_text", "start": 27, "stop": 227, "weights": [[5, 9, 1.0], [24, 30, 1.0], [31, 39, 1.0], [73, 77, 1.0]]}}], "run_2": [], "summary": [["2 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}, {"fields": {"query_id": "2", "title": "Information Retrieval", "description": "Which papers talk about Information Retrieval?", "narrative": "Relevant paper have at least one mention of Information Retrieval. Paper containing only Information or Retrieval are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [0.8], "P@10": [0.5], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [0.8687949224876582], "nDCG@10": [0.6274092458891772]}, "run_1": [{"doc_id": "2014.cikm_conference-2014.9", "score": 21.199382645837574, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[62, 71, 1.0], [81, 90, 1.0], [167, 178, 1.0], [179, 188, 1.0], [311, 320, 1.0], [417, 426, 1.0], [551, 560, 1.0], [643, 652, 1.0], [852, 861, 1.0], [871, 880, 1.0], [918, 927, 1.0], [1171, 1180, 1.0], [1322, 1331, 1.0], [1604, 1613, 1.0]]}, "snippet": {"field": "default_text", "start": 57, "stop": 257, "weights": [[5, 14, 1.0], [24, 33, 1.0], [110, 121, 1.0], [122, 131, 1.0]]}}, {"doc_id": "2003.sigirconf_conference-2003.73", "score": 20.673074350381277, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[4, 13, 1.0], [85, 96, 1.0], [97, 106, 1.0], [115, 124, 1.0], [140, 149, 1.0], [182, 191, 1.0], [238, 247, 1.0], [319, 328, 1.0], [506, 515, 1.0], [663, 672, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 13, 1.0], [85, 96, 1.0], [97, 106, 1.0], [115, 124, 1.0], [140, 149, 1.0], [182, 191, 1.0]]}}, {"doc_id": "2012.sigirjournals_journal-ir0anthology0volumeA46A1.9", "score": 20.650650415319788, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[11, 20, 1.0], [159, 170, 1.0], [171, 180, 1.0], [476, 485, 1.0], [528, 539, 1.0], [610, 619, 1.0], [749, 758, 1.0], [844, 853, 1.0], [872, 881, 1.0], [977, 988, 1.0], [1784, 1793, 1.0], [1823, 1832, 1.0], [1879, 1888, 1.0], [2021, 2030, 1.0], [2162, 2171, 1.0], [2464, 2473, 1.0], [2514, 2523, 1.0], [2579, 2590, 1.0], [2636, 2647, 1.0]]}, "snippet": {"field": "default_text", "start": 2459, "stop": 2659, "weights": [[5, 14, 1.0], [55, 64, 1.0], [120, 131, 1.0], [177, 188, 1.0]]}}, {"doc_id": "2015.ictir_conference-2015.2", "score": 20.62649034225412, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[10, 19, 1.0], [43, 54, 1.0], [168, 179, 1.0], [180, 189, 1.0], [668, 677, 1.0]]}, "snippet": {"field": "default_text", "start": 5, "stop": 205, "weights": [[5, 14, 1.0], [38, 49, 1.0], [163, 174, 1.0], [175, 184, 1.0]]}}, {"doc_id": "2005.ecir_conference-2005.13", "score": 19.712441571213944, "relevance": null, "rank": 5, "weights": {"doc_id": [], "default_text": [[10, 19, 1.0], [188, 199, 1.0], [200, 209, 1.0], [299, 308, 1.0], [420, 431, 1.0], [494, 503, 1.0], [1130, 1141, 1.0], [1142, 1151, 1.0]]}, "snippet": {"field": "default_text", "start": 5, "stop": 205, "weights": [[5, 14, 1.0], [183, 194, 1.0], [195, 204, 1.0]]}}, {"doc_id": "2006.clef_workshop-2006w.98", "score": 19.69653552343725, "relevance": null, "rank": 6, "weights": {"doc_id": [], "default_text": [[7, 16, 1.0], [39, 50, 1.0], [51, 60, 1.0], [137, 146, 1.0], [290, 301, 1.0], [302, 311, 1.0], [447, 456, 1.0]]}, "snippet": {"field": "default_text", "start": 2, "stop": 202, "weights": [[5, 14, 1.0], [37, 48, 1.0], [49, 58, 1.0], [135, 144, 1.0]]}}, {"doc_id": "2013.sigirjournals_journal-ir0anthology0volumeA47A1.11", "score": 19.689627131899428, "relevance": null, "rank": 7, "weights": {"doc_id": [], "default_text": [[19, 30, 1.0], [31, 40, 1.0], [42, 51, 1.0], [134, 145, 1.0], [188, 197, 1.0], [279, 290, 1.0], [291, 300, 1.0], [316, 325, 1.0], [557, 566, 1.0], [748, 757, 1.0], [962, 971, 1.0], [1041, 1050, 1.0], [1078, 1089, 1.0], [1160, 1171, 1.0], [1207, 1216, 1.0], [1289, 1300, 1.0], [1359, 1368, 1.0], [1402, 1411, 1.0]]}, "snippet": {"field": "default_text", "start": 14, "stop": 214, "weights": [[5, 16, 1.0], [17, 26, 1.0], [28, 37, 1.0], [120, 131, 1.0], [174, 183, 1.0]]}}, {"doc_id": "2018.trec_conference-2018.8", "score": 19.61271160778319, "relevance": null, "rank": 8, "weights": {"doc_id": [], "default_text": [[15, 24, 1.0], [59, 68, 1.0], [69, 78, 1.0], [322, 331, 1.0], [539, 548, 1.0], [784, 793, 1.0], [888, 897, 1.0]]}, "snippet": {"field": "default_text", "start": 10, "stop": 210, "weights": [[5, 14, 1.0], [49, 58, 1.0], [59, 68, 1.0]]}}, {"doc_id": "2005.ipm_journal-ir0anthology0volumeA41A2.1", "score": 19.193950440448226, "relevance": null, "rank": 9, "weights": {"doc_id": [], "default_text": [[32, 41, 1.0], [77, 86, 1.0], [155, 164, 1.0], [200, 209, 1.0], [337, 346, 1.0], [453, 462, 1.0], [1080, 1089, 1.0], [1162, 1171, 1.0]]}, "snippet": {"field": "default_text", "start": 27, "stop": 227, "weights": [[5, 14, 1.0], [50, 59, 1.0], [128, 137, 1.0], [173, 182, 1.0]]}}, {"doc_id": "2012.tist_journal-ir0anthology0volumeA3A4.10", "score": 19.165277943590297, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[0, 11, 1.0], [12, 21, 1.0], [64, 75, 1.0], [76, 85, 1.0], [260, 269, 1.0], [560, 569, 1.0], [636, 645, 1.0], [696, 707, 1.0], [730, 739, 1.0], [867, 878, 1.0], [879, 888, 1.0], [972, 981, 1.0], [1045, 1054, 1.0], [1104, 1113, 1.0], [1154, 1165, 1.0], [1166, 1175, 1.0]]}, "snippet": {"field": "default_text", "start": 967, "stop": 1167, "weights": [[5, 14, 1.0], [78, 87, 1.0], [137, 146, 1.0], [187, 198, 1.0], [199, 208, 1.0]]}}], "run_2": [], "summary": [["5 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}, {"fields": {"query_id": "3", "title": "Analysis for Android apps", "description": "Which papers talk about the analysis of android apps?", "narrative": "Relevant papers talk at least in one line about android apps. Paper containing only apps are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [0.8], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [0.8482378089219645]}, "run_1": [{"doc_id": "2018.wwwjournals_journal-ir0anthology0volumeA21A1.6", "score": 101.49764607765576, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[75, 82, 1.0], [94, 98, 1.0], [145, 149, 1.0], [173, 180, 1.0], [260, 264, 1.0], [614, 618, 1.0], [891, 895, 1.0], [954, 962, 1.0], [975, 983, 1.0], [1152, 1156, 1.0], [1520, 1524, 1.0], [1591, 1595, 1.0], [1664, 1672, 1.0], [1686, 1694, 1.0], [1729, 1733, 1.0]]}, "snippet": {"field": "default_text", "start": 70, "stop": 270, "weights": [[5, 12, 1.0], [24, 28, 1.0], [75, 79, 1.0], [103, 110, 1.0], [190, 194, 1.0]]}}, {"doc_id": "2020.tweb_journal-ir0anthology0volumeA14A3.4", "score": 97.92068716053848, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[49, 57, 1.0], [85, 92, 1.0], [93, 97, 1.0]]}, "snippet": {"field": "default_text", "start": 44, "stop": 244, "weights": [[5, 13, 1.0], [41, 48, 1.0], [49, 53, 1.0]]}}, {"doc_id": "2014.sigirconf_workshop-2014pir.5", "score": 97.80668304573356, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[13, 21, 1.0], [26, 33, 1.0], [34, 38, 1.0], [136, 140, 1.0]]}, "snippet": {"field": "default_text", "start": 8, "stop": 208, "weights": [[5, 13, 1.0], [18, 25, 1.0], [26, 30, 1.0], [128, 132, 1.0]]}}, {"doc_id": "2013.wwwconf_conference-2013.18", "score": 97.47038438457325, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[74, 81, 1.0], [233, 237, 1.0], [425, 432, 1.0], [433, 437, 1.0], [779, 783, 1.0], [862, 866, 1.0]]}, "snippet": {"field": "default_text", "start": 228, "stop": 428, "weights": [[5, 9, 1.0], [197, 204, 1.0]]}}, {"doc_id": "2017.wwwconf_conference-2017.18", "score": 95.19184500022051, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[72, 79, 1.0], [555, 563, 1.0], [648, 652, 1.0], [872, 876, 1.0]]}, "snippet": {"field": "default_text", "start": 550, "stop": 750, "weights": [[5, 13, 1.0], [98, 102, 1.0]]}}, {"doc_id": "2018.wwwjournals_journal-ir0anthology0volumeA21A1.5", "score": 89.49763665477047, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[30, 37, 1.0], [76, 84, 1.0], [102, 109, 1.0], [294, 298, 1.0], [401, 405, 1.0], [558, 562, 1.0], [691, 695, 1.0], [764, 768, 1.0], [1052, 1056, 1.0], [1174, 1178, 1.0], [1213, 1217, 1.0], [1316, 1320, 1.0], [1794, 1801, 1.0], [1802, 1806, 1.0]]}, "snippet": {"field": "default_text", "start": 25, "stop": 225, "weights": [[5, 12, 1.0], [51, 59, 1.0], [77, 84, 1.0]]}}, {"doc_id": "2018.wwwconf_conference-2018.144", "score": 88.22978456709907, "relevance": 0, "rank": 7, "weights": {"doc_id": [], "default_text": [[49, 56, 1.0], [642, 649, 1.0], [650, 654, 1.0], [757, 761, 1.0], [1131, 1135, 1.0], [1211, 1219, 1.0], [1236, 1244, 1.0], [1417, 1421, 1.0], [1585, 1592, 1.0], [1593, 1597, 1.0], [1607, 1615, 1.0]]}, "snippet": {"field": "default_text", "start": 1412, "stop": 1612, "weights": [[5, 9, 1.0], [173, 180, 1.0], [181, 185, 1.0], [195, 203, 1.0]]}}, {"doc_id": "2017.wwwconf_conference-2017.20", "score": 83.5891188925095, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[252, 256, 1.0], [450, 457, 1.0], [458, 462, 1.0], [520, 527, 1.0], [570, 578, 1.0], [705, 709, 1.0], [781, 785, 1.0], [936, 940, 1.0], [995, 999, 1.0], [1118, 1122, 1.0]]}, "snippet": {"field": "default_text", "start": 445, "stop": 645, "weights": [[5, 12, 1.0], [13, 17, 1.0], [75, 82, 1.0], [125, 133, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020.154", "score": 79.27566616721096, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[63, 70, 1.0], [71, 75, 1.0]]}, "snippet": {"field": "default_text", "start": 58, "stop": 258, "weights": [[5, 12, 1.0], [13, 17, 1.0]]}}, {"doc_id": "2015.wwwconf_conference-2015c.30", "score": 76.80068743886457, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[14, 22, 1.0], [164, 168, 1.0], [230, 234, 1.0], [580, 587, 1.0], [716, 720, 1.0], [794, 798, 1.0], [843, 847, 1.0], [947, 951, 1.0]]}, "snippet": {"field": "default_text", "start": 711, "stop": 911, "weights": [[5, 9, 1.0], [83, 87, 1.0], [132, 136, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "4", "title": "The University of Amsterdam", "description": "Which papers are created by the University of Amsterdam?", "narrative": "Relevant papers are created or at least co-written by the University of Amsterdam. Paper containing only\n      University not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [0.0], "P@3": [0.0], "P@5": [0.2], "P@10": [0.4], "nDCG@1": [0.0], "nDCG@3": [0.0], "nDCG@5": [0.13120507751234178], "nDCG@10": [0.28838205139432177]}, "run_1": [{"doc_id": "2012.trec_conference-2012.73", "score": 89.55910001536739, "relevance": 0, "rank": 1, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [78, 88, 1.0], [92, 101, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [78, 88, 1.0], [92, 101, 1.0]]}}, {"doc_id": "2005.clef_workshop-2005w.97", "score": 89.18462260411808, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [60, 70, 1.0], [74, 83, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [60, 70, 1.0], [74, 83, 1.0]]}}, {"doc_id": "2011.trec_conference-2011.102", "score": 85.25149330314156, "relevance": 0, "rank": 3, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [96, 106, 1.0], [110, 119, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [96, 106, 1.0], [110, 119, 1.0]]}}, {"doc_id": "2005.ntcir_workshop-2005.76", "score": 80.67624250094033, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [55, 65, 1.0], [69, 78, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [55, 65, 1.0], [69, 78, 1.0]]}}, {"doc_id": "2010.clef_workshop-2010w.104", "score": 77.73098825942904, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [94, 104, 1.0], [108, 117, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [94, 104, 1.0], [108, 117, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.205", "score": 77.57146448766466, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[0, 10, 1.0], [14, 23, 1.0], [62, 72, 1.0], [76, 85, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[0, 10, 1.0], [14, 23, 1.0], [62, 72, 1.0], [76, 85, 1.0]]}}, {"doc_id": "2014.sigirconf_workshop-2014erd.10", "score": 76.90646501984645, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[41, 51, 1.0], [55, 64, 1.0], [124, 134, 1.0], [138, 147, 1.0], [362, 372, 1.0], [376, 385, 1.0]]}, "snippet": {"field": "default_text", "start": 36, "stop": 236, "weights": [[5, 15, 1.0], [19, 28, 1.0], [88, 98, 1.0], [102, 111, 1.0]]}}, {"doc_id": "2008.clef_workshop-2008w.128", "score": 75.83074014301971, "relevance": null, "rank": 8, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [50, 60, 1.0], [64, 73, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [50, 60, 1.0], [64, 73, 1.0]]}}, {"doc_id": "2007.clef_workshop-2007w.112", "score": 75.39126874356222, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [128, 138, 1.0], [142, 151, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [128, 138, 1.0], [142, 151, 1.0]]}}, {"doc_id": "2015.trec_conference-2015.70", "score": 72.57021703241207, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[4, 14, 1.0], [18, 27, 1.0], [104, 114, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[4, 14, 1.0], [18, 27, 1.0], [104, 114, 1.0]]}}], "run_2": [], "summary": [["1 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}, {"fields": {"query_id": "5", "title": "Neural Ranking for eCommerce Product Search", "description": "Which paper is about Ranking Product Search and its applications in eCommerce?", "narrative": "Relevant papers that addresses application of Task Models and Textual Embeddings in the field of electronic\n      commerce. Not relevant are papers showcasing applications that do not contain the topic eCommerce.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [0.6666666666666666], "P@5": [0.4], "P@10": [0.4], "nDCG@1": [1.0], "nDCG@3": [0.7039180890341347], "nDCG@5": [0.5087403079104241], "nDCG@10": [0.49886024151151387]}, "run_1": [{"doc_id": "2018.sigirconf_workshop-2018ecom.14", "score": 147.19112654302296, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[11, 17, 1.0], [18, 25, 1.0], [30, 39, 1.0], [40, 47, 1.0], [48, 54, 1.0], [131, 137, 1.0], [138, 145, 1.0], [150, 159, 1.0], [160, 167, 1.0], [168, 174, 1.0], [270, 279, 1.0]]}, "snippet": {"field": "default_text", "start": 6, "stop": 206, "weights": [[5, 11, 1.0], [12, 19, 1.0], [24, 33, 1.0], [34, 41, 1.0], [42, 48, 1.0], [125, 131, 1.0], [132, 139, 1.0], [144, 153, 1.0], [154, 161, 1.0], [162, 168, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020c.44", "score": 110.57464697444183, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[13, 20, 1.0], [24, 33, 1.0], [34, 40, 1.0], [101, 110, 1.0], [111, 117, 1.0], [160, 167, 1.0], [439, 445, 1.0], [930, 936, 1.0]]}, "snippet": {"field": "default_text", "start": 8, "stop": 208, "weights": [[5, 12, 1.0], [16, 25, 1.0], [26, 32, 1.0], [93, 102, 1.0], [103, 109, 1.0], [152, 159, 1.0]]}}, {"doc_id": "2019.sigirconf_workshop-2019ecom.19", "score": 96.57642588484353, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[0, 7, 1.0], [23, 30, 1.0], [64, 70, 1.0], [86, 95, 1.0], [341, 347, 1.0], [525, 531, 1.0], [675, 682, 1.0], [694, 700, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[0, 7, 1.0], [23, 30, 1.0], [64, 70, 1.0], [86, 95, 1.0]]}}, {"doc_id": "2020.wsdm_conference-2020.13", "score": 91.54068814332724, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[54, 61, 1.0], [62, 68, 1.0], [95, 102, 1.0], [142, 151, 1.0], [210, 217, 1.0], [218, 224, 1.0], [385, 392, 1.0], [478, 485, 1.0], [486, 492, 1.0], [613, 619, 1.0], [850, 857, 1.0], [876, 883, 1.0], [974, 981, 1.0], [1248, 1255, 1.0], [1256, 1262, 1.0], [1310, 1316, 1.0]]}, "snippet": {"field": "default_text", "start": 49, "stop": 249, "weights": [[5, 12, 1.0], [13, 19, 1.0], [46, 53, 1.0], [93, 102, 1.0], [161, 168, 1.0], [169, 175, 1.0]]}}, {"doc_id": "2018.sigirconf_workshop-2018ecom.12", "score": 86.32577487462537, "relevance": 0, "rank": 5, "weights": {"doc_id": [], "default_text": [[0, 9, 1.0], [10, 17, 1.0], [315, 321, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[0, 9, 1.0], [10, 17, 1.0]]}}, {"doc_id": "2006.ipm_journal-ir0anthology0volumeA42A4.12", "score": 85.52850553759299, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[25, 31, 1.0], [64, 73, 1.0], [313, 322, 1.0], [397, 406, 1.0], [545, 551, 1.0], [589, 598, 1.0], [670, 676, 1.0], [700, 709, 1.0], [755, 764, 1.0], [830, 839, 1.0], [860, 866, 1.0], [958, 967, 1.0], [968, 974, 1.0], [1193, 1202, 1.0]]}, "snippet": {"field": "default_text", "start": 665, "stop": 865, "weights": [[5, 11, 1.0], [35, 44, 1.0], [90, 99, 1.0], [165, 174, 1.0], [195, 201, 1.0]]}}, {"doc_id": "2016.sigirconf_conference-2016.50", "score": 84.9643380615913, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[7, 13, 1.0], [26, 33, 1.0], [116, 122, 1.0], [217, 224, 1.0], [356, 363, 1.0], [364, 370, 1.0], [409, 416, 1.0], [575, 581, 1.0], [652, 659, 1.0], [713, 720, 1.0], [721, 727, 1.0], [867, 874, 1.0], [1018, 1025, 1.0], [1132, 1139, 1.0], [1244, 1250, 1.0], [1754, 1761, 1.0], [2814, 2821, 1.0], [3024, 3031, 1.0], [3519, 3525, 1.0], [3895, 3902, 1.0], [4215, 4222, 1.0], [4223, 4229, 1.0], [4490, 4496, 1.0], [4541, 4547, 1.0], [4714, 4720, 1.0], [4903, 4910, 1.0], [5088, 5095, 1.0]]}, "snippet": {"field": "default_text", "start": 212, "stop": 412, "weights": [[5, 12, 1.0], [144, 151, 1.0], [152, 158, 1.0], [197, 204, 1.0]]}}, {"doc_id": "2011.sigirconf_conference-2011.10", "score": 83.92300509195994, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[29, 38, 1.0], [95, 101, 1.0], [106, 115, 1.0], [117, 124, 1.0], [126, 132, 1.0], [154, 161, 1.0], [277, 284, 1.0], [354, 361, 1.0], [362, 368, 1.0], [462, 468, 1.0], [664, 673, 1.0], [674, 680, 1.0], [710, 719, 1.0]]}, "snippet": {"field": "default_text", "start": 24, "stop": 224, "weights": [[5, 14, 1.0], [71, 77, 1.0], [82, 91, 1.0], [93, 100, 1.0], [102, 108, 1.0], [130, 137, 1.0]]}}, {"doc_id": "2017.sigirconf_conference-2017.251", "score": 79.88728357740737, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[23, 32, 1.0], [501, 510, 1.0], [651, 660, 1.0], [735, 744, 1.0], [745, 751, 1.0], [890, 899, 1.0], [984, 990, 1.0], [1048, 1054, 1.0], [1128, 1134, 1.0]]}, "snippet": {"field": "default_text", "start": 646, "stop": 846, "weights": [[5, 14, 1.0], [89, 98, 1.0], [99, 105, 1.0]]}}, {"doc_id": "2019.spire_conference-2019.4", "score": 79.61711114988992, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[58, 67, 1.0], [68, 74, 1.0]]}, "snippet": {"field": "default_text", "start": 53, "stop": 253, "weights": [[5, 14, 1.0], [15, 21, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "6", "title": "web pages evolution", "description": "Which paper is about the evolution of web pages?", "narrative": "Relevant paper have at least one mention of web evolution. Paper containing only web page or evolution are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [0.8], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [0.8701249883466593]}, "run_1": [{"doc_id": "2005.wwwconf_workshop-2004webdyn.11", "score": 93.20566442996281, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[52, 61, 1.0], [73, 76, 1.0], [77, 82, 1.0], [137, 146, 1.0], [154, 157, 1.0], [292, 295, 1.0], [317, 320, 1.0], [321, 326, 1.0], [393, 402, 1.0], [513, 518, 1.0], [629, 634, 1.0], [741, 744, 1.0]]}, "snippet": {"field": "default_text", "start": 47, "stop": 247, "weights": [[5, 14, 1.0], [26, 29, 1.0], [30, 35, 1.0], [90, 99, 1.0], [107, 110, 1.0]]}}, {"doc_id": "2008.wwwconf_conference-2008.169", "score": 88.61605229178639, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[11, 20, 1.0], [24, 27, 1.0], [28, 33, 1.0], [85, 88, 1.0], [89, 94, 1.0], [274, 279, 1.0], [329, 332, 1.0]]}, "snippet": {"field": "default_text", "start": 6, "stop": 206, "weights": [[5, 14, 1.0], [18, 21, 1.0], [22, 27, 1.0], [79, 82, 1.0], [83, 88, 1.0]]}}, {"doc_id": "2005.wwwconf_conference-2005si.15", "score": 88.39565764754585, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[15, 24, 1.0], [28, 31, 1.0], [59, 64, 1.0], [161, 166, 1.0], [249, 258, 1.0], [301, 304, 1.0], [534, 537, 1.0]]}, "snippet": {"field": "default_text", "start": 10, "stop": 210, "weights": [[5, 14, 1.0], [18, 21, 1.0], [49, 54, 1.0], [151, 156, 1.0]]}}, {"doc_id": "2003.wwwconf_workshop-2002webdyn.1", "score": 81.63648531148877, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[0, 3, 1.0], [18, 27, 1.0], [28, 31, 1.0], [147, 150, 1.0], [188, 193, 1.0], [296, 299, 1.0], [399, 404, 1.0], [555, 558, 1.0]]}, "snippet": {"field": "default_text", "start": 0, "stop": 200, "weights": [[0, 3, 1.0], [18, 27, 1.0], [28, 31, 1.0], [147, 150, 1.0], [188, 193, 1.0]]}}, {"doc_id": "2003.wwwconf_conference-2003.70", "score": 79.60670369894467, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[27, 36, 1.0], [40, 43, 1.0], [44, 49, 1.0], [76, 79, 1.0], [203, 208, 1.0], [366, 369, 1.0], [560, 565, 1.0], [613, 618, 1.0], [695, 698, 1.0], [699, 704, 1.0], [758, 763, 1.0], [964, 969, 1.0], [1311, 1316, 1.0], [1563, 1568, 1.0], [1801, 1806, 1.0]]}, "snippet": {"field": "default_text", "start": 22, "stop": 222, "weights": [[5, 14, 1.0], [18, 21, 1.0], [22, 27, 1.0], [54, 57, 1.0], [181, 186, 1.0]]}}, {"doc_id": "2004.wwwconf_conference-2004.1", "score": 70.38799724045504, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[18, 21, 1.0], [28, 37, 1.0], [45, 48, 1.0], [132, 135, 1.0], [181, 184, 1.0], [319, 322, 1.0], [375, 384, 1.0], [504, 513, 1.0], [571, 576, 1.0], [609, 612, 1.0], [664, 669, 1.0], [767, 770, 1.0], [771, 776, 1.0], [901, 906, 1.0], [1253, 1258, 1.0], [1378, 1383, 1.0], [1570, 1573, 1.0]]}, "snippet": {"field": "default_text", "start": 13, "stop": 213, "weights": [[5, 8, 1.0], [15, 24, 1.0], [32, 35, 1.0], [119, 122, 1.0], [168, 171, 1.0]]}}, {"doc_id": "2005.cikm_conference-2005.139", "score": 67.69672526222072, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[11, 14, 1.0], [24, 33, 1.0], [81, 84, 1.0], [215, 218, 1.0], [279, 288, 1.0], [316, 319, 1.0], [320, 325, 1.0], [327, 330, 1.0], [456, 459, 1.0], [577, 582, 1.0], [784, 787, 1.0], [788, 793, 1.0]]}, "snippet": {"field": "default_text", "start": 210, "stop": 410, "weights": [[5, 8, 1.0], [69, 78, 1.0], [106, 109, 1.0], [110, 115, 1.0], [117, 120, 1.0]]}}, {"doc_id": "2008.wwwconf_conference-2008.205", "score": 66.20458075830724, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[34, 37, 1.0], [38, 43, 1.0], [97, 100, 1.0], [188, 191, 1.0], [211, 214, 1.0], [483, 492, 1.0], [496, 501, 1.0], [644, 649, 1.0]]}, "snippet": {"field": "default_text", "start": 29, "stop": 229, "weights": [[5, 8, 1.0], [9, 14, 1.0], [68, 71, 1.0], [159, 162, 1.0], [182, 185, 1.0]]}}, {"doc_id": "2013.wwwconf_conference-2013c.138", "score": 65.94201369963103, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[65, 68, 1.0], [69, 74, 1.0], [94, 103, 1.0], [111, 114, 1.0], [217, 220, 1.0], [221, 226, 1.0], [253, 256, 1.0], [257, 262, 1.0], [422, 425, 1.0], [426, 431, 1.0], [581, 584, 1.0], [585, 590, 1.0], [729, 738, 1.0], [742, 745, 1.0], [1061, 1064, 1.0], [1065, 1070, 1.0], [1158, 1161, 1.0], [1162, 1167, 1.0], [1208, 1211, 1.0], [1212, 1217, 1.0], [1319, 1328, 1.0]]}, "snippet": {"field": "default_text", "start": 60, "stop": 260, "weights": [[5, 8, 1.0], [9, 14, 1.0], [34, 43, 1.0], [51, 54, 1.0], [157, 160, 1.0], [161, 166, 1.0], [193, 196, 1.0], [197, 202, 1.0]]}}, {"doc_id": "2007.wwwconf_conference-2007.203", "score": 65.19945652316198, "relevance": 0, "rank": 10, "weights": {"doc_id": [], "default_text": [[33, 42, 1.0], [59, 62, 1.0], [93, 96, 1.0], [150, 153, 1.0], [487, 496, 1.0], [500, 503, 1.0], [581, 584, 1.0], [628, 637, 1.0], [659, 662, 1.0], [715, 718, 1.0]]}, "snippet": {"field": "default_text", "start": 482, "stop": 682, "weights": [[5, 14, 1.0], [18, 21, 1.0], [99, 102, 1.0], [146, 155, 1.0], [177, 180, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}], "docs": {"2019.sigirconf_workshop-2019ecom.19": {"doc_id": "2019.sigirconf_workshop-2019ecom.19", "default_text": "Ranking Sentences from Product Description & Bullets for Better Search Products in an ecommerce catalog contain information-rich fields like description and bullets that can be useful to extract entities (attributes) using NER based systems. However, these fields are often verbose and contain lot of information that is not relevant from a search perspective. Treating each sentence within these fields equally can lead to poor full text match and introduce problems in extracting attributes to develop ontologies, semantic search etc. To address this issue, we describe two methods based on extractive summarization with reinforcement learning by leveraging information in product titles and search click through logs to rank sentences from bullets, description, etc. Finally, we compare the precision of these two models.", "abstract": "Products in an ecommerce catalog contain information-rich fields like description and bullets that can be useful to extract entities (attributes) using NER based systems. However, these fields are often verbose and contain lot of information that is not relevant from a search perspective. Treating each sentence within these fields equally can lead to poor full text match and introduce problems in extracting attributes to develop ontologies, semantic search etc. To address this issue, we describe two methods based on extractive summarization with reinforcement learning by leveraging information in product titles and search click through logs to rank sentences from bullets, description, etc. Finally, we compare the precision of these two models.", "title": "Ranking Sentences from Product Description & Bullets for Better Search", "authors": ["Prateek Verma", "Aliasgar Kutiyanawala", "Ke Shen"], "year": "2019", "booktitle": "Proceedings of the SIGIR 2019 Workshop on eCommerce, co-located with the 42st International ACM SIGIR Conference on Research and Development in Information Retrieval, eCom@SIGIR 2019, Paris, France, July 25, 2019"}, "2014.sigirconf_workshop-2014erd.10": {"doc_id": "2014.sigirconf_workshop-2014erd.10", "default_text": "Semanticizing search engine queries: the University of Amsterdam at the ERD 2014 challenge ABSTRACTThis paper describes the University of Amsterdam's participation in the short track of the Entity Recognition & Disambiguation Challenge 2014 (ERD 2014). We describe how we adapt the Semanticizer-an open-source entity linking framework developed primarily at the University of Amsterdam-to the task of the ERD challenge: linking named entities in search engine queries. We steer the Semanticizer's linking towards named entities by adapting an existing training corpus, and extend the Semanticizer's set of features with contextual features that aim to leverage the limited context provided by search queries. With an F1 score of 0.6062 our final system run achieves median performance, and better than mean performance (0.5329).", "abstract": "ABSTRACTThis paper describes the University of Amsterdam's participation in the short track of the Entity Recognition & Disambiguation Challenge 2014 (ERD 2014). We describe how we adapt the Semanticizer-an open-source entity linking framework developed primarily at the University of Amsterdam-to the task of the ERD challenge: linking named entities in search engine queries. We steer the Semanticizer's linking towards named entities by adapting an existing training corpus, and extend the Semanticizer's set of features with contextual features that aim to leverage the limited context provided by search queries. With an F1 score of 0.6062 our final system run achieves median performance, and better than mean performance (0.5329).", "title": "Semanticizing search engine queries: the University of Amsterdam at the ERD 2014 challenge", "authors": ["David Graus", "Daan Odijk", "Manos Tsagkias", "Wouter Weerkamp", "Maarten de Rijke"], "year": "2014", "booktitle": "ERD'14, Proceedings of the First ACM International Workshop on Entity Recognition & Disambiguation, July 11, 2014, Gold Coast, Queensland, Australia"}, "2014.sigirconf_workshop-2014pir.5": {"doc_id": "2014.sigirconf_workshop-2014pir.5", "default_text": "User Comment Analysis for Android apps and CSPI Detection with Comment Expansion Along with the exponential growth on markets of mobile apps, comes the serious public concern about the security and privacy issues. User comments serves as a valuable source of information for evaluating a mobile app, for both new users and developers. However, for the purpose of evaluation on the security/privacy aspects of an app, user comments are not always directly useful. Most of the comments are about issues like functionality, missing feature or just pure emotional expression. Therefore, further efforts are required in order to identify those Comments with Security/Privacy Issues (CSPI) for future evaluation. In this paper, a dataset of comments is collected from Google Play, and a two dimensional label system is proposed to describe those CSPI within it. A supervised multi-label learning method utilizing comment expansion is adopted to detect different types of CSPI described by this label system. Experiments on the collected dataset shows that the proposed method outperforms the method without the comment expansion.", "abstract": "Along with the exponential growth on markets of mobile apps, comes the serious public concern about the security and privacy issues. User comments serves as a valuable source of information for evaluating a mobile app, for both new users and developers. However, for the purpose of evaluation on the security/privacy aspects of an app, user comments are not always directly useful. Most of the comments are about issues like functionality, missing feature or just pure emotional expression. Therefore, further efforts are required in order to identify those Comments with Security/Privacy Issues (CSPI) for future evaluation. In this paper, a dataset of comments is collected from Google Play, and a two dimensional label system is proposed to describe those CSPI within it. A supervised multi-label learning method utilizing comment expansion is adopted to detect different types of CSPI described by this label system. Experiments on the collected dataset shows that the proposed method outperforms the method without the comment expansion.", "title": "User Comment Analysis for Android apps and CSPI Detection with Comment Expansion", "authors": ["Lei Cen", "Luo Si", "Ninghui Li", "Hongxia Jin"], "year": "2014", "booktitle": "Proceeding of the 1st International Workshop on Privacy-Preserving IR: When Information Retrieval Meets Privacy and Security co-located with 37th Annual International ACM SIGIR conference, PIR@SIGIR 2014, Gold Coast, Australia, July 11, 2014"}, "2018.sigirconf_workshop-2018ecom.12": {"doc_id": "2018.sigirconf_workshop-2018ecom.12", "default_text": "Ecommerce Product Title Classification E-commerce catalogs include a continuously growing number of products that are constantly updated. Each item in a catalog is characterized by several attributes and identified by a taxonomy label. Categorizing products with their taxonomy labels is fundamental to effectively search and organize listings in a catalog. However, manual and/or rule based approaches to categorization are not scalable. In this paper, we explain our work for the SIGIR eCom'18 Rakuten Data Challenge [1] which focuses on the Topic of largescale taxonomy classification. We first start with data processing. Secondly we investigate a number of feature extraction techniques and observe that TF-IDF with both bigram and unigram work best for categorization than CNN and word embedding. Finally, we evaluate several models and find than Support Vector Machines yield the highest result.", "abstract": "E-commerce catalogs include a continuously growing number of products that are constantly updated. Each item in a catalog is characterized by several attributes and identified by a taxonomy label. Categorizing products with their taxonomy labels is fundamental to effectively search and organize listings in a catalog. However, manual and/or rule based approaches to categorization are not scalable. In this paper, we explain our work for the SIGIR eCom'18 Rakuten Data Challenge [1] which focuses on the Topic of largescale taxonomy classification. We first start with data processing. Secondly we investigate a number of feature extraction techniques and observe that TF-IDF with both bigram and unigram work best for categorization than CNN and word embedding. Finally, we evaluate several models and find than Support Vector Machines yield the highest result.", "title": "Ecommerce Product Title Classification", "authors": ["Sylvain Goumy", "Mohamed-Amine Mejri"], "year": "2018", "booktitle": "The SIGIR 2018 Workshop On eCommerce co-located with the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2018), Ann Arbor, Michigan, USA, July 12, 2018"}, "2018.sigirconf_workshop-2018ecom.14": {"doc_id": "2018.sigirconf_workshop-2018ecom.14", "default_text": "End-to-End Neural Ranking for eCommerce Product Search: an Application of Task Models and Textual Embeddings Yan. 2018. End-to-End Neural Ranking for eCommerce Product Search: An application of task models and textual embeddings. In Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR 2018 eCom). ACM,", "abstract": "Yan. 2018. End-to-End Neural Ranking for eCommerce Product Search: An application of task models and textual embeddings. In Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR 2018 eCom). ACM,", "title": "End-to-End Neural Ranking for eCommerce Product Search: an Application of Task Models and Textual Embeddings", "authors": ["Eliot Brenner", "Jun Zhao", "Aliasgar Kutiyanawala", "Zheng Yan"], "year": "2018", "booktitle": "The SIGIR 2018 Workshop On eCommerce co-located with the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2018), Ann Arbor, Michigan, USA, July 12, 2018"}, "2018.sigirconf_workshop-2018ecom.33": {"doc_id": "2018.sigirconf_workshop-2018ecom.33", "default_text": "Visualizing and Understanding Deep Neural Networks in CTR Prediction Although deep learning techniques have been successfully applied to many tasks, interpreting deep neural network models is still a big challenge to us. Recently, many works have been done on visualizing and analyzing the mechanism of deep neural networks in the areas of image processing and natural language processing. In this paper, we present our approaches to visualize and understand deep neural networks for a very important commercial task-CTR (Click-through rate) prediction. We conduct experiments on the productive data from our online advertising system with daily varying distribution. To understand the mechanism and the performance of the model, we inspect the model's inner status at neuron level. Also, a probe approach is implemented to measure the layer-wise performance of the model. Moreover, to measure the influence from the input features, we calculate saliency scores based on the backpropagated gradients. Practical applications are also discussed, for example, in understanding, monitoring, diagnosing and refining models and algorithms.", "abstract": "Although deep learning techniques have been successfully applied to many tasks, interpreting deep neural network models is still a big challenge to us. Recently, many works have been done on visualizing and analyzing the mechanism of deep neural networks in the areas of image processing and natural language processing. In this paper, we present our approaches to visualize and understand deep neural networks for a very important commercial task-CTR (Click-through rate) prediction. We conduct experiments on the productive data from our online advertising system with daily varying distribution. To understand the mechanism and the performance of the model, we inspect the model's inner status at neuron level. Also, a probe approach is implemented to measure the layer-wise performance of the model. Moreover, to measure the influence from the input features, we calculate saliency scores based on the backpropagated gradients. Practical applications are also discussed, for example, in understanding, monitoring, diagnosing and refining models and algorithms.", "title": "Visualizing and Understanding Deep Neural Networks in CTR Prediction", "authors": ["Lin Guo", "Hui Ye", "Wenbo Su", "Henhuan Liu", "Kai Sun", "Hang Xiang"], "year": "2018", "booktitle": "The SIGIR 2018 Workshop On eCommerce co-located with the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2018), Ann Arbor, Michigan, USA, July 12, 2018"}, "2005.ntcir_workshop-2005.76": {"doc_id": "2005.ntcir_workshop-2005.76", "default_text": "The University of Amsterdam at NTCIR-5 We describe the University of Amsterdam's participation in the Cross-Lingual Information Retrieval task at NTCIR-5. We focused on Chinese monolingual retrieval, and aimed to study the effectiveness of language models and different tokenization methods for Chinese. Our main findings are the following. First, where the vector space model excels on a bigram index, the language model performs poorly. Second, on a unigram index, the language model is very effective, and even exceeds the performance of the vector space model on the bigram index. Third, and at a more technical level, in comparison to word-based langauges such as English we found that language models for Chinese require less smoothing, due to the different indexing unit.", "abstract": "We describe the University of Amsterdam's participation in the Cross-Lingual Information Retrieval task at NTCIR-5. We focused on Chinese monolingual retrieval, and aimed to study the effectiveness of language models and different tokenization methods for Chinese. Our main findings are the following. First, where the vector space model excels on a bigram index, the language model performs poorly. Second, on a unigram index, the language model is very effective, and even exceeds the performance of the vector space model on the bigram index. Third, and at a more technical level, in comparison to word-based langauges such as English we found that language models for Chinese require less smoothing, due to the different indexing unit.", "title": "The University of Amsterdam at NTCIR-5", "authors": ["Jaap Kamps", "Michiel van der Bruggen", "Maarten de Rijke"], "year": "2005", "booktitle": "Proceedings of the Fifth NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-5, National Center of Sciences, Tokyo, Japan, December 6-9, 2005"}, "2010.clef_workshop-2010w.104": {"doc_id": "2010.clef_workshop-2010w.104", "default_text": "The University of Amsterdam's Concept Detection System at ImageCLEF 2010 Our group within the University of Amsterdam participated in the large-scale visual concept detection task of ImageCLEF 2010. The submissions from our visual concept detection system have resulted in the best visual-only run in the per-concept evaluation. In the per-image evaluation, it achieves the highest score in terms of example-based F-measure across all types of runs.", "abstract": "Our group within the University of Amsterdam participated in the large-scale visual concept detection task of ImageCLEF 2010. The submissions from our visual concept detection system have resulted in the best visual-only run in the per-concept evaluation. In the per-image evaluation, it achieves the highest score in terms of example-based F-measure across all types of runs.", "title": "The University of Amsterdam's Concept Detection System at ImageCLEF 2010", "authors": ["Koen E. A. van de Sande", "Theo Gevers"], "year": "2010", "booktitle": "CLEF 2010 LABs and Workshops, Notebook Papers, 22-23 September 2010, Padua, Italy"}, "2008.clef_workshop-2008w.128": {"doc_id": "2008.clef_workshop-2008w.128", "default_text": "The University of Amsterdam at VideoCLEF 2008 The University of Amsterdam (UAms) team carried out the Vid2RSS classification task, the primary sub-task of the VideoCLEF track at CLEF 2008. This task involves the assignment of thematic category labels to dual language (Dutch/English) television episode videos. UAms chose to focus on exploiting archival metadata and speech transcripts generated by both the Dutch and English speech recognizers. Exploratory experimentation completed prior to the start of the task on external data motivated choosing a Support Vector Machine (SVM) with a linear kernel as the classifier. As a SVM toolbox to carry out the experiments, the Least Square-SVM (LS-SVM) toolbox was selected. Wikipedia was chosen as the source of the training data because it is multilingual and contains content with broad thematic coverage. The results of the experimentation showed that archival metadata improves performance of classification, but the addition of speech recognition transcripts in one or both languages does not yield performance gains. Although the overall performance of the classifiers was less than satisfactory, adequate performance was achieved in several classes, suggesting that there is concrete potential for future work to achieve performance improvements, especially if more suitable training data could be obtained.", "abstract": "The University of Amsterdam (UAms) team carried out the Vid2RSS classification task, the primary sub-task of the VideoCLEF track at CLEF 2008. This task involves the assignment of thematic category labels to dual language (Dutch/English) television episode videos. UAms chose to focus on exploiting archival metadata and speech transcripts generated by both the Dutch and English speech recognizers. Exploratory experimentation completed prior to the start of the task on external data motivated choosing a Support Vector Machine (SVM) with a linear kernel as the classifier. As a SVM toolbox to carry out the experiments, the Least Square-SVM (LS-SVM) toolbox was selected. Wikipedia was chosen as the source of the training data because it is multilingual and contains content with broad thematic coverage. The results of the experimentation showed that archival metadata improves performance of classification, but the addition of speech recognition transcripts in one or both languages does not yield performance gains. Although the overall performance of the classifiers was less than satisfactory, adequate performance was achieved in several classes, suggesting that there is concrete potential for future work to achieve performance improvements, especially if more suitable training data could be obtained.", "title": "The University of Amsterdam at VideoCLEF 2008", "authors": ["Jiyin He", "Xu Zhang", "Wouter Weerkamp", "Martha A. Larson"], "year": "2008", "booktitle": "Working Notes for CLEF 2008 Workshop co-located with the 12th European Conference on Digital Libraries (ECDL 2008) , Aarhus, Denmark, September 17-19, 2008"}, "2005.clef_workshop-2005w.97": {"doc_id": "2005.clef_workshop-2005w.97", "default_text": "The University of Amsterdam at WebCLEF 2005 We describe the University of Amsterdam's participation in the WebCLEF track at CLEF 2005. We submitted runs for both the mixed monolingual task and the multilingual task.", "abstract": "We describe the University of Amsterdam's participation in the WebCLEF track at CLEF 2005. We submitted runs for both the mixed monolingual task and the multilingual task.", "title": "The University of Amsterdam at WebCLEF 2005", "authors": ["Jaap Kamps", "Maarten de Rijke", "B\u00f6rkur Sigurbj\u00f6rnsson"], "year": "2005", "booktitle": "Working Notes for CLEF 2005 Workshop co-located with the 9th European Conference on Digital Libraries (ECDL 2005), Wien, Austria, September 21-22, 2005"}, "2006.clef_workshop-2006w.98": {"doc_id": "2006.clef_workshop-2006w.98", "default_text": "Speech Retrieval Experiments using XML Information Retrieval This report presents the University of Twente's first cross-language speech retrieval experiments in Cross-Language Evaluation Forum (CLEF). It describes the issues our contribution was focusing on, it describes the PF/Tijah XML Information Retrieval system that was used and it discusses the results for both the monolingual English and the Dutch-English crosslanguage spoken document retrieval (CL-SR) task. The paper concludes with an overview of future research plans.", "abstract": "This report presents the University of Twente's first cross-language speech retrieval experiments in Cross-Language Evaluation Forum (CLEF). It describes the issues our contribution was focusing on, it describes the PF/Tijah XML Information Retrieval system that was used and it discusses the results for both the monolingual English and the Dutch-English crosslanguage spoken document retrieval (CL-SR) task. The paper concludes with an overview of future research plans.", "title": "Speech Retrieval Experiments using XML Information Retrieval", "authors": ["Djoerd Hiemstra", "Roeland Ordelman", "Robin Aly", "Laurens van der Werff", "Franciska de Jong"], "year": "2006", "booktitle": "Working Notes for CLEF 2006 Workshop co-located with the 10th European Conference on Digital Libraries (ECDL 2006), Alicante, Spain, September 20-22, 2006"}, "2007.clef_workshop-2007w.112": {"doc_id": "2007.clef_workshop-2007w.112", "default_text": "The University of Amsterdam at the CLEF Cross Language Speech Retrieval Track 2007 In this paper we present the contents of the University of Amsterdam submission in the CLEF Cross Language Speech Retrieval 2007 English task. We describe the effects of using character n-grams and field combinations on both monolingual English retrieval, and crosslingual Dutch to English retrieval.", "abstract": "In this paper we present the contents of the University of Amsterdam submission in the CLEF Cross Language Speech Retrieval 2007 English task. We describe the effects of using character n-grams and field combinations on both monolingual English retrieval, and crosslingual Dutch to English retrieval.", "title": "The University of Amsterdam at the CLEF Cross Language Speech Retrieval Track 2007", "authors": ["Bouke Huurnink"], "year": "2007", "booktitle": "Working Notes for CLEF 2007 Workshop co-located with the 11th European Conference on Digital Libraries (ECDL 2007), Budapest, Hungary, September 19-21, 2007"}, "2003.wwwconf_workshop-2002webdyn.1": {"doc_id": "2003.wwwconf_workshop-2002webdyn.1", "default_text": "Web Structure and Evolution Web Structure, Age, and Page Quality This paper is aimed at the study of quantitative measures of the relation between Web structure, age, and quality o f W eb pages. Quality is studied from di erent link-based metrics and their relationship with the structure of the Web and the last modi cation time of a page. We show that, as expected, Pagerank is biased against new pages. As a subproduct we propose a P agerank variant that includes age into account a n d w e obtain information on how the rate of change is related with Web structure.", "abstract": "This paper is aimed at the study of quantitative measures of the relation between Web structure, age, and quality o f W eb pages. Quality is studied from di erent link-based metrics and their relationship with the structure of the Web and the last modi cation time of a page. We show that, as expected, Pagerank is biased against new pages. As a subproduct we propose a P agerank variant that includes age into account a n d w e obtain information on how the rate of change is related with Web structure.", "title": "Web Structure and Evolution Web Structure, Age, and Page Quality", "authors": ["Ricardo A. Baeza-Yates", "Felipe Saint-Jean", "Carlos Castillo"], "year": "2002", "booktitle": "Proceedings of the Second International Workshop on Web Dynamics, WebDyn@WWW 2002, Honululu, HW, USA, May 7, 2002"}, "2005.wwwconf_workshop-2004webdyn.11": {"doc_id": "2005.wwwconf_workshop-2004webdyn.11", "default_text": "WebRelievo: A System for Browsing and Analyzing the Evolution of Related Web Pages WebRelievo is a system for browsing and analyzing the evolution of the web graph structure based on link analysis. This system enables us to answer historical questions, and to detect changes in topics on the Web. WebRelievo extracts web pages related to a focused page using link analysis, and visualizes the evolution of their relationships with a time series of graphs. This visualization enables us to understand when related pages appeared, and how their relationships have evolved over time. The user can interactively browse those related pages by changing the focused page and by changing layouts of graphs. WebRelievo is implemented on six Japanese web archives crawled from 1999 to 2003.", "abstract": "WebRelievo is a system for browsing and analyzing the evolution of the web graph structure based on link analysis. This system enables us to answer historical questions, and to detect changes in topics on the Web. WebRelievo extracts web pages related to a focused page using link analysis, and visualizes the evolution of their relationships with a time series of graphs. This visualization enables us to understand when related pages appeared, and how their relationships have evolved over time. The user can interactively browse those related pages by changing the focused page and by changing layouts of graphs. WebRelievo is implemented on six Japanese web archives crawled from 1999 to 2003.", "title": "WebRelievo: A System for Browsing and Analyzing the Evolution of Related Web Pages", "authors": ["Masashi Toyoda", "Masaru Kitsuregawa"], "year": "2004", "booktitle": "Proceedings of the Third International Workshop on Web Dynamics, WebDyn@WWW 2004, New York, NY, USA, May 18, 2004"}, "2011.sigirconf_conference-2011.10": {"doc_id": "2011.sigirconf_conference-2011.10", "default_text": "User behavior in zero-recall ecommerce queries ABSTRACTUser expectation and experience for web search and eCommerce (product) search are quite different. Product descriptions are concise as compared to typical web documents. User expectation is more specific to find the right product. The difference in the publisher and searcher vocabulary (in case of product search the seller and the buyer vocabulary) combined with the fact that there are fewer products to search over than web documents result in observable numbers of searches that return no results (zero recall searches). In this paper we describe a study of zero recall searches. Our study is focused on eCommerce search and uses data from a leading eCommerce site's user click stream logs. There are 3 main contributions of our study: 1) The cause of zero recall searches; 2) A study of user's reaction and recovery from zero recall; 3) A study of differences in behavior of power users versus novice users to zero recall searches.", "abstract": "ABSTRACTUser expectation and experience for web search and eCommerce (product) search are quite different. Product descriptions are concise as compared to typical web documents. User expectation is more specific to find the right product. The difference in the publisher and searcher vocabulary (in case of product search the seller and the buyer vocabulary) combined with the fact that there are fewer products to search over than web documents result in observable numbers of searches that return no results (zero recall searches). In this paper we describe a study of zero recall searches. Our study is focused on eCommerce search and uses data from a leading eCommerce site's user click stream logs. There are 3 main contributions of our study: 1) The cause of zero recall searches; 2) A study of user's reaction and recovery from zero recall; 3) A study of differences in behavior of power users versus novice users to zero recall searches.", "title": "User behavior in zero-recall ecommerce queries", "authors": ["Gyanit Singh", "Nish Parikh", "Neel Sundaresan"], "year": "2011", "booktitle": "Proceeding of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011"}, "2015.sigirconf_conference-2015.135": {"doc_id": "2015.sigirconf_conference-2015.135", "default_text": "Twitter Sentiment Analysis with Deep Convolutional Neural Networks ABSTRACTThis paper describes our deep learning system for sentiment analysis of tweets. The main contribution of this work is a new model for initializing the parameter weights of the convolutional neural network, which is crucial to train an accurate model while avoiding the need to inject any additional features. Briefly, we use an unsupervised neural language model to train initial word embeddings that are further tuned by our deep learning model on a distant supervised corpus. At a final stage, the pre-trained parameters of the network are used to initialize the model. We train the latter on the supervised training data recently made available by the official system evaluation campaign on Twitter Sentiment Analysis organized by Semeval-2015.  A comparison between the results of our approach and the systems participating in the challenge on the official test sets, suggests that our model could be ranked in the first two positions in both the phrase-level subtask A (among 11 teams) and on the message-level subtask B (among 40 teams). This is an important evidence on the practical value of our solution.", "abstract": "ABSTRACTThis paper describes our deep learning system for sentiment analysis of tweets. The main contribution of this work is a new model for initializing the parameter weights of the convolutional neural network, which is crucial to train an accurate model while avoiding the need to inject any additional features. Briefly, we use an unsupervised neural language model to train initial word embeddings that are further tuned by our deep learning model on a distant supervised corpus. At a final stage, the pre-trained parameters of the network are used to initialize the model. We train the latter on the supervised training data recently made available by the official system evaluation campaign on Twitter Sentiment Analysis organized by Semeval-2015.  A comparison between the results of our approach and the systems participating in the challenge on the official test sets, suggests that our model could be ranked in the first two positions in both the phrase-level subtask A (among 11 teams) and on the message-level subtask B (among 40 teams). This is an important evidence on the practical value of our solution.", "title": "Twitter Sentiment Analysis with Deep Convolutional Neural Networks", "authors": ["Aliaksei Severyn", "Alessandro Moschitti"], "year": "2015", "booktitle": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015"}, "2016.sigirconf_conference-2016.50": {"doc_id": "2016.sigirconf_conference-2016.50", "default_text": "Amazon Search: The Joy of Ranking Products ABSTRACTAmazon is one of the world's largest e-commerce sites and Amazon Search powers the majority of Amazon's sales. As a consequence, even small improvements in relevance ranking both positively influence the shopping experience of millions of customers and significantly impact revenue. In the past, Amazon's product search engine consisted of several handtuned ranking functions using a handful of input features. A lot has changed since then. In this talk we are going to cover a number of relevance algorithms used in Amazon Search today. We will describe a general machine learning framework used for ranking within categories, blending separate rankings in All Product Search, NLP techniques used for matching queries and products, and algorithms targeted at unique tasks of specific categories -books and fashion.\nRANKING MODELSRanking models are responsible for a function that, given a customer's query, returns a sorted list of products in a match set. A single ranking model usually covers a combination of a category and a marketplace, e.g., Books in Japan.For training the ranking models we use labels based on customers actions, such as purchases, add-to-basket, or clicks.We use the search engine to collect our training sets. Several times per day we compute the unique set of keywords issued for each context of interest. The context can be a combination of marketplace, category, and some user features. Then we re-issue these queries in their context requesting feature values for all items in the match set. This feature collection runs regularly, so that the feature vector collected is as close as possible to the one observed when the query was originally issued by customers.To train ranking models, we construct training, validation, and test sets by collecting data from several days of customer traffic. Test sets are constructed from dates after the training set dates. We choose impressions that resulted in either click or purchase as positive examples. There are two Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).SIGIR '16 July 17-21, 2016, Pisa, Italy     types of negative example impressions: seen, corresponding to items which were displayed to a customer, and unseen, corresponding to items which matched the query terms but were never shown due to pagination. To manage the size of the training set, we sample unseen examples.Gradient boosted trees[1] are our method of choice in ranking because they can discover complex feature interactions, can handle categorical and real-valued features, are robust in the presence of missing values, and work well even without significant tuning. For ranking problems, we use models trained with pairwise objectives and nDCG as the default objective function.We do feature selection in two stages. First, with fixed values of tree depth and learning rate, we train a \"kitchen sink\" model allowing all features available. We then discard the features which end up being ranked lower than random features and use the remaining features to grow the final feature set in a forward selection process. Once the feature set is chosen, we perform a grid search over the pairs of values for tree depth and learning rate, choosing the model which has the highest offline score on the validation set.Finally, the model is evaluated through an A/B test. We look at a large variety of metrics, including the number of converting customers, the number of products they purchase and the overall revenue.\nBEHAVIORAL FEATURESWhen training ranking models we use many features. Some of them measure intrinsic properties of products (e.g., sales, reviews). Others reflect properties of the queries or the context in which the query is issued (e.g., query specificity, customer status). Other features provide different measures of textual similarity. However in product search, hundreds of products might share very similar descriptions and seem to be equally relevant to a particular query. But some of those products are more popular than others and should be ranked higher. That's why behavioral features drive the rankings in Amazon Search to a much larger extent than they do in Web Search. Typically, they account for most of the variance reduction in gradient-boosted trees.It is well-known that users tend to click more often on results on the top of a search result page. To correct for that, we have tried to use classical versions of click-overexpected-clicks features . However, since more relevant documents tend to appear higher in the ranking, the observed click-through rate at a given position captures not only the position bias, but also the typical relevance at this position. This problem is more pronounced for a product 459", "abstract": "ABSTRACTAmazon is one of the world's largest e-commerce sites and Amazon Search powers the majority of Amazon's sales. As a consequence, even small improvements in relevance ranking both positively influence the shopping experience of millions of customers and significantly impact revenue. In the past, Amazon's product search engine consisted of several handtuned ranking functions using a handful of input features. A lot has changed since then. In this talk we are going to cover a number of relevance algorithms used in Amazon Search today. We will describe a general machine learning framework used for ranking within categories, blending separate rankings in All Product Search, NLP techniques used for matching queries and products, and algorithms targeted at unique tasks of specific categories -books and fashion.\nRANKING MODELSRanking models are responsible for a function that, given a customer's query, returns a sorted list of products in a match set. A single ranking model usually covers a combination of a category and a marketplace, e.g., Books in Japan.For training the ranking models we use labels based on customers actions, such as purchases, add-to-basket, or clicks.We use the search engine to collect our training sets. Several times per day we compute the unique set of keywords issued for each context of interest. The context can be a combination of marketplace, category, and some user features. Then we re-issue these queries in their context requesting feature values for all items in the match set. This feature collection runs regularly, so that the feature vector collected is as close as possible to the one observed when the query was originally issued by customers.To train ranking models, we construct training, validation, and test sets by collecting data from several days of customer traffic. Test sets are constructed from dates after the training set dates. We choose impressions that resulted in either click or purchase as positive examples. There are two Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).SIGIR '16 July 17-21, 2016, Pisa, Italy     types of negative example impressions: seen, corresponding to items which were displayed to a customer, and unseen, corresponding to items which matched the query terms but were never shown due to pagination. To manage the size of the training set, we sample unseen examples.Gradient boosted trees[1] are our method of choice in ranking because they can discover complex feature interactions, can handle categorical and real-valued features, are robust in the presence of missing values, and work well even without significant tuning. For ranking problems, we use models trained with pairwise objectives and nDCG as the default objective function.We do feature selection in two stages. First, with fixed values of tree depth and learning rate, we train a \"kitchen sink\" model allowing all features available. We then discard the features which end up being ranked lower than random features and use the remaining features to grow the final feature set in a forward selection process. Once the feature set is chosen, we perform a grid search over the pairs of values for tree depth and learning rate, choosing the model which has the highest offline score on the validation set.Finally, the model is evaluated through an A/B test. We look at a large variety of metrics, including the number of converting customers, the number of products they purchase and the overall revenue.\nBEHAVIORAL FEATURESWhen training ranking models we use many features. Some of them measure intrinsic properties of products (e.g., sales, reviews). Others reflect properties of the queries or the context in which the query is issued (e.g., query specificity, customer status). Other features provide different measures of textual similarity. However in product search, hundreds of products might share very similar descriptions and seem to be equally relevant to a particular query. But some of those products are more popular than others and should be ranked higher. That's why behavioral features drive the rankings in Amazon Search to a much larger extent than they do in Web Search. Typically, they account for most of the variance reduction in gradient-boosted trees.It is well-known that users tend to click more often on results on the top of a search result page. To correct for that, we have tried to use classical versions of click-overexpected-clicks features . However, since more relevant documents tend to appear higher in the ranking, the observed click-through rate at a given position captures not only the position bias, but also the typical relevance at this position. This problem is more pronounced for a product 459", "title": "Amazon Search: The Joy of Ranking Products", "authors": ["Daria Sorokina", "Erick Cant\u00fa-Paz"], "year": "2016", "booktitle": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016"}, "2016.sigirconf_conference-2016.150": {"doc_id": "2016.sigirconf_conference-2016.150", "default_text": "Quote Recommendation in Dialogue using Deep Neural Network ABSTRACTQuotes, or quotations, are well known phrases or sentences that we use for various purposes such as emphasis, elaboration, and humor. In this paper, we introduce a task of recommending quotes which are suitable for given dialogue context and we present a deep learning recommender system which combines recurrent neural network and convolutional neural network in order to learn semantic representation of each utterance and construct a sequence model for the dialog thread. We collected a large set of twitter dialogues with quote occurrences in order to evaluate proposed recommender system. Experimental results show that our approach outperforms not only the other state-of-the-art algorithms in quote recommendation task, but also other neural network based methods built for similar tasks.", "abstract": "ABSTRACTQuotes, or quotations, are well known phrases or sentences that we use for various purposes such as emphasis, elaboration, and humor. In this paper, we introduce a task of recommending quotes which are suitable for given dialogue context and we present a deep learning recommender system which combines recurrent neural network and convolutional neural network in order to learn semantic representation of each utterance and construct a sequence model for the dialog thread. We collected a large set of twitter dialogues with quote occurrences in order to evaluate proposed recommender system. Experimental results show that our approach outperforms not only the other state-of-the-art algorithms in quote recommendation task, but also other neural network based methods built for similar tasks.", "title": "Quote Recommendation in Dialogue using Deep Neural Network", "authors": ["Hanbit Lee", "Yeonchan Ahn", "Haejun Lee", "Seungdo Ha", "Sang-goo Lee"], "year": "2016", "booktitle": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016"}, "2017.sigirconf_conference-2017.251": {"doc_id": "2017.sigirconf_conference-2017.251", "default_text": "SIGIR 2017 Workshop on eCommerce (ECOM17) ABSTRACTeCommerce Information Retrieval has received little attention in the academic literature, yet it is an essential component of some of the largest web sites (such as eBay, Amazon, Airbnb, Alibaba, Taobao, Target, Facebook, and others). SIGIR has for several years seen sponsorship from these kinds of organisations, who clearly value the importance of research into Information Retrieval. This workshop brings together researchers and practitioners of eCommerce IR to discuss topics unique to it, to set a research agenda, and to examine how to build a dataset for research into this fascinating topic.eCommerce IR is ripe for research and has a unique set of problems. For example, in eCommerce search there may be no hypertext links between documents (products); there is a click stream, but more importantly, there is often a buy stream. eCommerce problems are wide in scope and range from user interaction modalities (the kinds of search seen in when buying are different from those of web-page search (i.e. it is not clear how shopping and buying relate to the standard web-search interaction models)) through to dynamic updates of a rapidly changing collection on auction sites, and the experienceness of some products (such as Airbnb bookings).", "abstract": "ABSTRACTeCommerce Information Retrieval has received little attention in the academic literature, yet it is an essential component of some of the largest web sites (such as eBay, Amazon, Airbnb, Alibaba, Taobao, Target, Facebook, and others). SIGIR has for several years seen sponsorship from these kinds of organisations, who clearly value the importance of research into Information Retrieval. This workshop brings together researchers and practitioners of eCommerce IR to discuss topics unique to it, to set a research agenda, and to examine how to build a dataset for research into this fascinating topic.eCommerce IR is ripe for research and has a unique set of problems. For example, in eCommerce search there may be no hypertext links between documents (products); there is a click stream, but more importantly, there is often a buy stream. eCommerce problems are wide in scope and range from user interaction modalities (the kinds of search seen in when buying are different from those of web-page search (i.e. it is not clear how shopping and buying relate to the standard web-search interaction models)) through to dynamic updates of a rapidly changing collection on auction sites, and the experienceness of some products (such as Airbnb bookings).", "title": "SIGIR 2017 Workshop on eCommerce (ECOM17)", "authors": ["Jon Degenhardt", "Surya Kallumadi", "Maarten de Rijke", "Luo Si", "Andrew Trotman", "Yinghui Xu"], "year": "2017", "booktitle": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017"}, "2003.sigirconf_conference-2003.73": {"doc_id": "2003.sigirconf_conference-2003.73", "default_text": "XML retrieval: what to retrieve? ABSTRACTThe fundamental difference between standard information retrieval and XML retrieval is the unit of retrieval. In traditional IR, the unit of retrieval is fixed: it is the complete document. In XML retrieval, every XML element in a document is a retrievable unit. This makes XML retrieval more difficult: besides being relevant, a retrieved unit should be neither too large nor too small. The research presented here, a comparative analysis of two approaches to XML retrieval, aims to shed light on which XML elements should be retrieved. The experimental evaluation uses data from the Initiative for the Evaluation of XML retrieval (INEX 2002).", "abstract": "ABSTRACTThe fundamental difference between standard information retrieval and XML retrieval is the unit of retrieval. In traditional IR, the unit of retrieval is fixed: it is the complete document. In XML retrieval, every XML element in a document is a retrievable unit. This makes XML retrieval more difficult: besides being relevant, a retrieved unit should be neither too large nor too small. The research presented here, a comparative analysis of two approaches to XML retrieval, aims to shed light on which XML elements should be retrieved. The experimental evaluation uses data from the Initiative for the Evaluation of XML retrieval (INEX 2002).", "title": "XML retrieval: what to retrieve?", "authors": ["Jaap Kamps", "Maarten Marx", "Maarten de Rijke", "B\u00f6rkur Sigurbj\u00f6rnsson"], "year": "2003", "booktitle": "SIGIR 2003: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, July 28 - August 1, 2003, Toronto, Canada"}, "2020.clef_conference-2020w.205": {"doc_id": "2020.clef_conference-2020w.205", "default_text": "University of Amsterdam at CLEF 2020 This paper documents the University of Amsterdam's participation in CLEF 2020 Touch\u00e9 Track. This is the first year this track has been introduced at CLEF, and we were attracted to participate in it due to its potentialities for Parliamentary debates we are currently working on. This track consists of two tasks: Conversational Argument Retrieval and Comparative Argument Retrieval. We submitted a run to both tasks. For the first task, we used a combination of the traditional BM25 model and learning to rank models. BM25 model helps to retrieve relevant arguments, and learning to rank model helps to re-rank the list and put stronger arguments on top of the list. For the second task, Comparative Argument Retrieval, we proposed a pipeline to re-rank documents retrieved from Clueweb using three features: PageRank scores, web domains, and argumentativeness. Preliminary results on 5 queries have shown that this heuristic pipeline may help to achieve a balance among three important dimensions: relevance, trustworthiness, and argumentativeness.", "abstract": "This paper documents the University of Amsterdam's participation in CLEF 2020 Touch\u00e9 Track. This is the first year this track has been introduced at CLEF, and we were attracted to participate in it due to its potentialities for Parliamentary debates we are currently working on. This track consists of two tasks: Conversational Argument Retrieval and Comparative Argument Retrieval. We submitted a run to both tasks. For the first task, we used a combination of the traditional BM25 model and learning to rank models. BM25 model helps to retrieve relevant arguments, and learning to rank model helps to re-rank the list and put stronger arguments on top of the list. For the second task, Comparative Argument Retrieval, we proposed a pipeline to re-rank documents retrieved from Clueweb using three features: PageRank scores, web domains, and argumentativeness. Preliminary results on 5 queries have shown that this heuristic pipeline may help to achieve a balance among three important dimensions: relevance, trustworthiness, and argumentativeness.", "title": "University of Amsterdam at CLEF 2020", "authors": ["Mahsa S. Shahshahani", "Jaap Kamps"], "year": "2020", "booktitle": "Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020"}, "2020.wsdm_conference-2020.4": {"doc_id": "2020.wsdm_conference-2020.4", "default_text": "Can Deep Learning Only Be Neural Networks? The word \"deep learning\" is generally regarded as a synonym of \"deep neural networks (DNNs)\". In this talk, we will discuss on essentials in deep learning and claim that deep learning is not necessarily to be realized by neural networks and differentiable modules. We will then present an exploration to non-NN style deep learning, where the building blocks are non-differentiable modules and the training process does not rely on backpropagation or gradient-based adjustment. We will also talk about some recent advances and challenges in this direction of research.", "abstract": "The word \"deep learning\" is generally regarded as a synonym of \"deep neural networks (DNNs)\". In this talk, we will discuss on essentials in deep learning and claim that deep learning is not necessarily to be realized by neural networks and differentiable modules. We will then present an exploration to non-NN style deep learning, where the building blocks are non-differentiable modules and the training process does not rely on backpropagation or gradient-based adjustment. We will also talk about some recent advances and challenges in this direction of research.", "title": "Can Deep Learning Only Be Neural Networks?", "authors": ["Zhi-Hua Zhou"], "year": "2020", "booktitle": "WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020"}, "2020.wsdm_conference-2020.13": {"doc_id": "2020.wsdm_conference-2020.13", "default_text": "Why Do People Buy Seemingly Irrelevant Items in Voice Product Search?: On the Relation between Product Relevance and Customer Satisfaction in eCommerce One emerging benefit of voice assistants is to facilitate product search experience, allowing users to express orally which products they seek, and taking actions on retrieved results such as adding them to their cart or sending the product details to their mobile phone for further examination. Looking at users' behavior in product search, supported by a digital voice assistant, we have observed an interesting phenomenon where users purchase or engage with search results that are objectively judged irrelevant to their queries. In this work, we analyze and characterize this phenomenon. We provide several hypotheses as to the reasons behind it, including users' personalized preferences, the product's popularity, the product's indirect relation with the query, the user's tolerance level, the query intent, and the product price. We address each hypothesis by conducting thorough data analyses and offer some insights with respect to users' purchase and engagement behavior with seemingly irrelevant results. We conclude with a discussion on how this analysis can be used to improve voice product search services. CCS CONCEPTS \u2022 Information systems \u2192 Search interfaces.", "abstract": "One emerging benefit of voice assistants is to facilitate product search experience, allowing users to express orally which products they seek, and taking actions on retrieved results such as adding them to their cart or sending the product details to their mobile phone for further examination. Looking at users' behavior in product search, supported by a digital voice assistant, we have observed an interesting phenomenon where users purchase or engage with search results that are objectively judged irrelevant to their queries. In this work, we analyze and characterize this phenomenon. We provide several hypotheses as to the reasons behind it, including users' personalized preferences, the product's popularity, the product's indirect relation with the query, the user's tolerance level, the query intent, and the product price. We address each hypothesis by conducting thorough data analyses and offer some insights with respect to users' purchase and engagement behavior with seemingly irrelevant results. We conclude with a discussion on how this analysis can be used to improve voice product search services. CCS CONCEPTS \u2022 Information systems \u2192 Search interfaces.", "title": "Why Do People Buy Seemingly Irrelevant Items in Voice Product Search?: On the Relation between Product Relevance and Customer Satisfaction in eCommerce", "authors": ["David Carmel", "Elad Haramaty", "Arnon Lazerson", "Liane Lewin-Eytan", "Yoelle Maarek"], "year": "2020", "booktitle": "WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020"}, "2020.wsdm_conference-2020.126": {"doc_id": "2020.wsdm_conference-2020.126", "default_text": "Decision Boundary of Deep Neural Networks: Challenges and Opportunities One crucial aspect that yet remains fairly unknown while can inform us about the behavior of deep neural networks is their decision boundaries. Trust can be improved once we understand how and why deep models carve out a particular form of decision boundary and thus make particular decisions. Robustness against adversarial examples is directly related to the decision boundary as adversarial examples are basically 'missed out' by the decision boundary between two classes. Investigating the decision boundary of deep neural networks, nevertheless, faces tremendous challenges. First, how we can generate instances near the decision boundary that are similar to real samples? Second, how we can leverage near decision boundary instances to characterize the behaviour of deep neural networks? Motivated to solve these challenges, we focus on investigating the decision boundary of deep neural network classifiers. In particular, we propose a novel approach to generate instances near decision boundary of pre-trained DNNs and then leverage these instances to characterize the behaviour of deep models. CCS CONCEPTS \u2022 Computing methodologies \u2192 Neural networks; Causal reasoning and diagnostics; Computer vision; Supervised learning.", "abstract": "One crucial aspect that yet remains fairly unknown while can inform us about the behavior of deep neural networks is their decision boundaries. Trust can be improved once we understand how and why deep models carve out a particular form of decision boundary and thus make particular decisions. Robustness against adversarial examples is directly related to the decision boundary as adversarial examples are basically 'missed out' by the decision boundary between two classes. Investigating the decision boundary of deep neural networks, nevertheless, faces tremendous challenges. First, how we can generate instances near the decision boundary that are similar to real samples? Second, how we can leverage near decision boundary instances to characterize the behaviour of deep neural networks? Motivated to solve these challenges, we focus on investigating the decision boundary of deep neural network classifiers. In particular, we propose a novel approach to generate instances near decision boundary of pre-trained DNNs and then leverage these instances to characterize the behaviour of deep models. CCS CONCEPTS \u2022 Computing methodologies \u2192 Neural networks; Causal reasoning and diagnostics; Computer vision; Supervised learning.", "title": "Decision Boundary of Deep Neural Networks: Challenges and Opportunities", "authors": ["Hamid Karimi", "Jiliang Tang"], "year": "2020", "booktitle": "WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020"}, "2005.ecir_conference-2005.13": {"doc_id": "2005.ecir_conference-2005.13", "default_text": "Improving Retrieval Effectiveness by Using Key Terms in Top Retrieved Documents Abstract. In this paper, we propose a method to improve the precision of top retrieved documents in Chinese information retrieval where the query is a short description by re-ordering retrieved documents in the initial retrieval. To reorder the documents, we firstly find out terms in query and their importance scales by making use of the information derived from top N (N<=30) retrieved documents in the initial retrieval; secondly, we re-order retrieved K (N<<K) documents by what kinds of terms of query they contain. That is, we first automatically extract key terms from top N retrieved documents, then we collect key terms that occur in query and their document frequencies in the N retrieved documents, finally we use these collected terms to re-order the initially retrieved documents. Each collected term is assigned a weight by its length and its document frequency in top N retrieved documents. Each document is re-ranked by the sum of weights of collected terms it contains. In our experiments on 42 query topics in NTCIR3 Cross Lingual Information Retrieval (CLIR) dataset, an average 17.8%-27.5% improvement can be made for top 10 documents and an average 6.6%-26.9% improvement can be made for top 100 documents at relax/rigid relevance judgment and different parameter setting.", "abstract": "Abstract. In this paper, we propose a method to improve the precision of top retrieved documents in Chinese information retrieval where the query is a short description by re-ordering retrieved documents in the initial retrieval. To reorder the documents, we firstly find out terms in query and their importance scales by making use of the information derived from top N (N<=30) retrieved documents in the initial retrieval; secondly, we re-order retrieved K (N<<K) documents by what kinds of terms of query they contain. That is, we first automatically extract key terms from top N retrieved documents, then we collect key terms that occur in query and their document frequencies in the N retrieved documents, finally we use these collected terms to re-order the initially retrieved documents. Each collected term is assigned a weight by its length and its document frequency in top N retrieved documents. Each document is re-ranked by the sum of weights of collected terms it contains. In our experiments on 42 query topics in NTCIR3 Cross Lingual Information Retrieval (CLIR) dataset, an average 17.8%-27.5% improvement can be made for top 10 documents and an average 6.6%-26.9% improvement can be made for top 100 documents at relax/rigid relevance judgment and different parameter setting.", "title": "Improving Retrieval Effectiveness by Using Key Terms in Top Retrieved Documents", "authors": ["Lingpeng Yang", "Dong-Hong Ji", "Guodong Zhou", "Nie Yu"], "year": "2005", "booktitle": "Advances in Information Retrieval, 27th European Conference on IR Research, ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005, Proceedings"}, "2014.cikm_conference-2014.9": {"doc_id": "2014.cikm_conference-2014.9", "default_text": "A Retrievability Analysis: Exploring the Relationship Between Retrieval Bias and Retrieval Performance ABSTRACTRetrievability provides an alternative way to assess an Information Retrieval (IR) system by measuring how easily documents can be retrieved. Retrievability can also be used to determine the level of retrieval bias a system exerts upon a collection of documents. It has been hypothesised that reducing the retrieval bias will lead to improved performance. To date, it has been shown that this hypothesis does not appear to hold on standard retrieval performance measures (MAP and P@10) when exploring the parameter space of a given retrieval model. However, the evidence is limited and confined to only a few models, collections and measures. In this paper, we perform a comprehensive empirical evaluation analysing the relationship between retrieval bias and retrieval performance using several well known retrieval models, five large TREC test collections and ten performance measures (including the recently proposed PRES, Time Biased Gain (TBG) and U-Measure). For traditional relevance based measures (MAP, P@10, MRR, Recall, etc) the correlation between retrieval bias and performance is moderate. However, for TBG and U-Measure, we find that there is strong and significant negative correlations between retrieval bias and performance (i.e as bias drops, performance increases). These findings suggest that for these more sophisticated, user oriented measures the retrievability bias hypothesis tends to hold. The implication is that for these measures, systems can then be tuned using retrieval bias, without recourse to relevance judgements.", "abstract": "ABSTRACTRetrievability provides an alternative way to assess an Information Retrieval (IR) system by measuring how easily documents can be retrieved. Retrievability can also be used to determine the level of retrieval bias a system exerts upon a collection of documents. It has been hypothesised that reducing the retrieval bias will lead to improved performance. To date, it has been shown that this hypothesis does not appear to hold on standard retrieval performance measures (MAP and P@10) when exploring the parameter space of a given retrieval model. However, the evidence is limited and confined to only a few models, collections and measures. In this paper, we perform a comprehensive empirical evaluation analysing the relationship between retrieval bias and retrieval performance using several well known retrieval models, five large TREC test collections and ten performance measures (including the recently proposed PRES, Time Biased Gain (TBG) and U-Measure). For traditional relevance based measures (MAP, P@10, MRR, Recall, etc) the correlation between retrieval bias and performance is moderate. However, for TBG and U-Measure, we find that there is strong and significant negative correlations between retrieval bias and performance (i.e as bias drops, performance increases). These findings suggest that for these more sophisticated, user oriented measures the retrievability bias hypothesis tends to hold. The implication is that for these measures, systems can then be tuned using retrieval bias, without recourse to relevance judgements.", "title": "A Retrievability Analysis: Exploring the Relationship Between Retrieval Bias and Retrieval Performance", "authors": ["Colin Wilkie", "Leif Azzopardi"], "year": "2014", "booktitle": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014"}, "2005.cikm_conference-2005.139": {"doc_id": "2005.cikm_conference-2005.139", "default_text": "Effects of web document evolution on genre classification ABSTRACTThe World Wide Web is a massive corpus that constantly evolves. Classification experiments usually grab a snapshot (temporally and spatially) of the Web for a corpus. In this paper, we examine the effects of page evolution on genre classification of Web pages. Web genre refers to the type of the page characterized by features such as style, form or presentation layout, and meta-content; Web genre can be used to tune spider crawling re-visits and inform relevance judgments for search engines. We found that pages in some genres change rarely if at all and can be used in present-day research experiments without requiring an updated version. We show that an old corpus can be used for training when testing on new Web pages, with only a marginal drop in accuracy rates on genre classification. We also show that features found to be useful in one corpus do not transfer well to other corpora with different genres.", "abstract": "ABSTRACTThe World Wide Web is a massive corpus that constantly evolves. Classification experiments usually grab a snapshot (temporally and spatially) of the Web for a corpus. In this paper, we examine the effects of page evolution on genre classification of Web pages. Web genre refers to the type of the page characterized by features such as style, form or presentation layout, and meta-content; Web genre can be used to tune spider crawling re-visits and inform relevance judgments for search engines. We found that pages in some genres change rarely if at all and can be used in present-day research experiments without requiring an updated version. We show that an old corpus can be used for training when testing on new Web pages, with only a marginal drop in accuracy rates on genre classification. We also show that features found to be useful in one corpus do not transfer well to other corpora with different genres.", "title": "Effects of web document evolution on genre classification", "authors": ["Elizabeth Sugar Boese", "Adele E. Howe"], "year": "2005", "booktitle": "Proceedings of the 2005 ACM CIKM International Conference on Information and Knowledge Management, Bremen, Germany, October 31 - November 5, 2005"}, "2017.cikm_conference-2017.151": {"doc_id": "2017.cikm_conference-2017.151", "default_text": "BayDNN: Friend Recommendation with Bayesian Personalized Ranking Deep Neural Network ABSTRACTFriend recommendation is a critical task in social networks. In this paper, we propose a Bayesian Personalized Ranking Deep Neural Network (BayDNN) model for friend recommendation in social networks. BayDNN rst extracts latent structural pa erns from the input network data and then use the Bayesian ranking to make friend recommendations. With BayDNN we achieve signi cant performance improvement on two public datasets: Epinions and Slashdot. For example, on Epinions dataset, BayDNN signi cantly outperforms the state-of-the-art algorithms, with a 5% improvement on NDCG over the best baseline. e advantages of the proposed BayDNN mainly come from a novel Bayesian personalized ranking (BPR) idea, which precisely captures the users' personal bias based on the extracted deep features, and its underlying convolutional neural network (CNN), which o ers a mechanism to extract latent deep structural feature representations of the complicated network data. To get good parameter estimation for the neural network, we present a netuned pre-training strategy for the proposed BayDNN model based on Poisson and Bernoulli probabilistic models.", "abstract": "ABSTRACTFriend recommendation is a critical task in social networks. In this paper, we propose a Bayesian Personalized Ranking Deep Neural Network (BayDNN) model for friend recommendation in social networks. BayDNN rst extracts latent structural pa erns from the input network data and then use the Bayesian ranking to make friend recommendations. With BayDNN we achieve signi cant performance improvement on two public datasets: Epinions and Slashdot. For example, on Epinions dataset, BayDNN signi cantly outperforms the state-of-the-art algorithms, with a 5% improvement on NDCG over the best baseline. e advantages of the proposed BayDNN mainly come from a novel Bayesian personalized ranking (BPR) idea, which precisely captures the users' personal bias based on the extracted deep features, and its underlying convolutional neural network (CNN), which o ers a mechanism to extract latent deep structural feature representations of the complicated network data. To get good parameter estimation for the neural network, we present a netuned pre-training strategy for the proposed BayDNN model based on Poisson and Bernoulli probabilistic models.", "title": "BayDNN: Friend Recommendation with Bayesian Personalized Ranking Deep Neural Network", "authors": ["Daizong Ding", "Mi Zhang", "Shao-Yuan Li", "Jie Tang", "Xiaotie Chen", "Zhi-Hua Zhou"], "year": "2017", "booktitle": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017"}, "2017.cikm_conference-2017.271": {"doc_id": "2017.cikm_conference-2017.271", "default_text": "Deep Neural Networks for News Recommendations ABSTRACTA fundamental role of news websites is to recommend articles that are interesting to read. The key challenge of news recommendation is to recommend newly published articles. Unlike other domains, outdated items are considered to be irrelevant in the news recommendation task. Another challenge is that the recommendation candidates are not seen in the training phase. In this paper, we introduce deep neural network models to overcome these challenges. we propose a modified session-based Recurrent Neural Network (RNN) model tailored to news recommendation as well as a historybased RNN model that spans the whole user's past histories. Finally, we propose a Convolutional Neural Network (CNN) model to capture user preferences and to personalize recommendation results. Experimental results on real-world news dataset shows that our model outperforms competitive baselines.", "abstract": "ABSTRACTA fundamental role of news websites is to recommend articles that are interesting to read. The key challenge of news recommendation is to recommend newly published articles. Unlike other domains, outdated items are considered to be irrelevant in the news recommendation task. Another challenge is that the recommendation candidates are not seen in the training phase. In this paper, we introduce deep neural network models to overcome these challenges. we propose a modified session-based Recurrent Neural Network (RNN) model tailored to news recommendation as well as a historybased RNN model that spans the whole user's past histories. Finally, we propose a Convolutional Neural Network (CNN) model to capture user preferences and to personalize recommendation results. Experimental results on real-world news dataset shows that our model outperforms competitive baselines.", "title": "Deep Neural Networks for News Recommendations", "authors": ["Keunchan Park", "Jisoo Lee", "Jaeho Choi"], "year": "2017", "booktitle": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017"}, "2017.cikm_conference-2017.312": {"doc_id": "2017.cikm_conference-2017.312", "default_text": "Spectrum-based Deep Neural Networks for Fraud Detection ABSTRACTIn this paper, we focus on fraud detection on a signed graph with only a small set of labeled training data. We propose a novel framework that combines deep neural networks and spectral graph analysis. In particular, we use the node projection (called as spectral coordinate) in the low dimensional spectral space of the graph's adjacency matrix as the input of deep neural networks. Spectral coordinates in the spectral space capture the most useful topology information of the network. Due to the small dimension of spectral coordinates (compared with the dimension of the adjacency matrix derived from a graph), training deep neural networks becomes feasible. We develop and evaluate two neural networks, deep autoencoder and convolutional neural network, in our fraud detection framework. Experimental results on a real signed graph show that our spectrum based deep neural networks are e ective in fraud detection.", "abstract": "ABSTRACTIn this paper, we focus on fraud detection on a signed graph with only a small set of labeled training data. We propose a novel framework that combines deep neural networks and spectral graph analysis. In particular, we use the node projection (called as spectral coordinate) in the low dimensional spectral space of the graph's adjacency matrix as the input of deep neural networks. Spectral coordinates in the spectral space capture the most useful topology information of the network. Due to the small dimension of spectral coordinates (compared with the dimension of the adjacency matrix derived from a graph), training deep neural networks becomes feasible. We develop and evaluate two neural networks, deep autoencoder and convolutional neural network, in our fraud detection framework. Experimental results on a real signed graph show that our spectrum based deep neural networks are e ective in fraud detection.", "title": "Spectrum-based Deep Neural Networks for Fraud Detection", "authors": ["Shuhan Yuan", "Xintao Wu", "Jun Li", "Aidong Lu"], "year": "2017", "booktitle": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017"}, "2019.spire_conference-2019.4": {"doc_id": "2019.spire_conference-2019.4", "default_text": "Position Bias Estimation for Unbiased Learning-to-Rank in eCommerce Search ", "abstract": "", "title": "Position Bias Estimation for Unbiased Learning-to-Rank in eCommerce Search", "authors": ["Grigor Aslanyan", "Utkarsh Porwal"], "year": "2019", "booktitle": "String Processing and Information Retrieval - 26th International Symposium, SPIRE 2019, Segovia, Spain, October 7-9, 2019, Proceedings"}, "2015.ictir_conference-2015.2": {"doc_id": "2015.ictir_conference-2015.2", "default_text": "Theory of Retrieval: The Retrievability of Information ABSTRACTRetrievability is an important and interesting indicator that can be used in a number of ways to analyse Information Retrieval systems and document collections. Rather than focusing totally on relevance, retrievability examines what is retrieved, how often it is retrieved, and whether a user is likely to retrieve it or not. This is important because a document needs to be retrieved, before it can be judged for relevance. In this tutorial, we shall explain the concept of retrievability along with a number of retrievability measures, how it can be estimated and how it can be used for analysis. Since retrieval precedes relevance, we shall also provide an overview of how retrievability relates to effectivenessdescribing some of the insights that researchers have discovered thus far. We shall also show how retrievability relates to efficiency, and how the theory of retrievability can be used to improve both effectiveness and efficiency. Then we shall provide an overview of the different applications of retrievability such as Search Engine Bias, Corpus Profiling, etc., before wrapping up with challenges and opportunities. The final session will look at example problems and ways to analyse and apply retrievability to other problems and domains. Participants are invited to bring their own problems to be discussed after the tutorial. This half-day tutorial is ideal for: (i) researchers curious about retrievability and wanting to see how it can impact their research, (ii) researchers who would like to expand their set of analysis techniques, and/or (iii) researchers who would like to use retrievability to perform their own analysis.", "abstract": "ABSTRACTRetrievability is an important and interesting indicator that can be used in a number of ways to analyse Information Retrieval systems and document collections. Rather than focusing totally on relevance, retrievability examines what is retrieved, how often it is retrieved, and whether a user is likely to retrieve it or not. This is important because a document needs to be retrieved, before it can be judged for relevance. In this tutorial, we shall explain the concept of retrievability along with a number of retrievability measures, how it can be estimated and how it can be used for analysis. Since retrieval precedes relevance, we shall also provide an overview of how retrievability relates to effectivenessdescribing some of the insights that researchers have discovered thus far. We shall also show how retrievability relates to efficiency, and how the theory of retrievability can be used to improve both effectiveness and efficiency. Then we shall provide an overview of the different applications of retrievability such as Search Engine Bias, Corpus Profiling, etc., before wrapping up with challenges and opportunities. The final session will look at example problems and ways to analyse and apply retrievability to other problems and domains. Participants are invited to bring their own problems to be discussed after the tutorial. This half-day tutorial is ideal for: (i) researchers curious about retrievability and wanting to see how it can impact their research, (ii) researchers who would like to expand their set of analysis techniques, and/or (iii) researchers who would like to use retrievability to perform their own analysis.", "title": "Theory of Retrieval: The Retrievability of Information", "authors": ["Leif Azzopardi"], "year": "2015", "booktitle": "Proceedings of the 2015 International Conference on The Theory of Information Retrieval, ICTIR 2015, Northampton, Massachusetts, USA, September 27-30, 2015"}, "2007.wwwconf_conference-2007.203": {"doc_id": "2007.wwwconf_conference-2007.203", "default_text": "Mirror site maintenance based on evolution associations of web directories ABSTRACTMirroring Web sites is a well-known technique commonly used in the Web community. A mirror site should be updated frequently to ensure that it reflects the content of the original site. Existing mirroring tools apply page-level strategies to check each page of a site, which is inefficient and expensive. In this paper, we propose a novel site-level mirror maintenance strategy. Our approach studies the evolution of Web directory structures and mines association rules between ancestor-descendant Web directories. Discovered rules indicate the evolution correlations between Web directories. Thus, when maintaining the mirror of a Web site (directory), we can optimally skip subdirectories which are negatively correlated with it in undergoing significant changes. The preliminary experimental results show that our approach improves the efficiency of the mirror maintenance process significantly while sacrificing slightly in keeping the \"freshness\" of the mirrors.", "abstract": "ABSTRACTMirroring Web sites is a well-known technique commonly used in the Web community. A mirror site should be updated frequently to ensure that it reflects the content of the original site. Existing mirroring tools apply page-level strategies to check each page of a site, which is inefficient and expensive. In this paper, we propose a novel site-level mirror maintenance strategy. Our approach studies the evolution of Web directory structures and mines association rules between ancestor-descendant Web directories. Discovered rules indicate the evolution correlations between Web directories. Thus, when maintaining the mirror of a Web site (directory), we can optimally skip subdirectories which are negatively correlated with it in undergoing significant changes. The preliminary experimental results show that our approach improves the efficiency of the mirror maintenance process significantly while sacrificing slightly in keeping the \"freshness\" of the mirrors.", "title": "Mirror site maintenance based on evolution associations of web directories", "authors": ["Ling Chen", "Sourav S. Bhowmick", "Wolfgang Nejdl"], "year": "2007", "booktitle": "Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007"}, "2015.wwwconf_conference-2015c.30": {"doc_id": "2015.wwwconf_conference-2015c.30", "default_text": "A Descriptive Analysis of a Large-Scale Collection of App Management Activities ABSTRACTSmartphone users adopt an increasing number of mobile applications (a.k.a., apps) in the recent years. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behaviors and preferences of mobile users. Existing literature provides very limited understanding about app management activities, due to the lack of user behavioral data at scale. This paper analyzes a very large collection of app management log of the users of a leading Android app marketplace in China. The data set covers one month of detailed activities of how users download, update, and uninstall the apps on their smart devices, involving 8,306,181 anonymized users and 394,661 apps. We characterize how these users manage the apps on their devices and identify behavioral patterns that correlate with users' online ratings of the apps.", "abstract": "ABSTRACTSmartphone users adopt an increasing number of mobile applications (a.k.a., apps) in the recent years. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behaviors and preferences of mobile users. Existing literature provides very limited understanding about app management activities, due to the lack of user behavioral data at scale. This paper analyzes a very large collection of app management log of the users of a leading Android app marketplace in China. The data set covers one month of detailed activities of how users download, update, and uninstall the apps on their smart devices, involving 8,306,181 anonymized users and 394,661 apps. We characterize how these users manage the apps on their devices and identify behavioral patterns that correlate with users' online ratings of the apps.", "title": "A Descriptive Analysis of a Large-Scale Collection of App Management Activities", "authors": ["Huoran Li", "Xuanzhe Liu", "Wei Ai", "Qiaozhu Mei", "Feng Feng"], "year": "2015", "booktitle": "Proceedings of the 24th International Conference on World Wide Web Companion, WWW 2015, Florence, Italy, May 18-22, 2015 - Companion Volume"}, "2008.wwwconf_conference-2008.169": {"doc_id": "2008.wwwconf_conference-2008.169", "default_text": "Microscale evolution of web pages ABSTRACTWe track a large set of \"rapidly\" changing web pages and examine the assumption that the arrival of content changes follows a Poisson process on a microscale. We demonstrate that there are significant differences in the behavior of pages that can be exploited to maintain freshness in a web corpus.", "abstract": "ABSTRACTWe track a large set of \"rapidly\" changing web pages and examine the assumption that the arrival of content changes follows a Poisson process on a microscale. We demonstrate that there are significant differences in the behavior of pages that can be exploited to maintain freshness in a web corpus.", "title": "Microscale evolution of web pages", "authors": ["Carrie Grimes"], "year": "2008", "booktitle": "Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008"}, "2008.wwwconf_conference-2008.205": {"doc_id": "2008.wwwconf_conference-2008.205", "default_text": "Visualizing historical content of web pages ABSTRACTRecently, along with the rapid growth of the Web, the preservation efforts have also increased. As a consequence, large amounts of past Web data are stored in Web archives. This historical data can be used for better understanding of long-term page topics and characteristics. In this paper, we propose an interactive visualization system called Page History Explorer for exploring page histories. It allows for roughly portraying evolution of pages and summarizing their content over time. We use a temporal term cloud as a structure for visualizing prevailing and active terms appearing on pages in the past.", "abstract": "ABSTRACTRecently, along with the rapid growth of the Web, the preservation efforts have also increased. As a consequence, large amounts of past Web data are stored in Web archives. This historical data can be used for better understanding of long-term page topics and characteristics. In this paper, we propose an interactive visualization system called Page History Explorer for exploring page histories. It allows for roughly portraying evolution of pages and summarizing their content over time. We use a temporal term cloud as a structure for visualizing prevailing and active terms appearing on pages in the past.", "title": "Visualizing historical content of web pages", "authors": ["Adam Jatowt", "Yukiko Kawai", "Katsumi Tanaka"], "year": "2008", "booktitle": "Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008"}, "2020.wwwconf_conference-2020c.44": {"doc_id": "2020.wwwconf_conference-2020c.44", "default_text": "Personalized Ranking in eCommerce Search We address the problem of personalization in the context of eCommerce search. Specifically, we develop personalization ranking features that use in-session context to augment a generic ranker optimized for conversion and relevance. We use a combination of latent features learned from item co-clicks in historic sessions and content based features that use item title and price. Personalization in search has been discussed extensively in the existing literature. The novelty of our work is combining and comparing content based and content agnostic features and showing that they complement each other to result in a significant improvement of the ranker. We experimentally show that our technique significantly outperforms a generic ranker in terms of Mean Reciprocal Rank (MRR). We also provide anecdotal evidence for the semantic similarity captured by the item embeddings on the eBay search engine.", "abstract": "We address the problem of personalization in the context of eCommerce search. Specifically, we develop personalization ranking features that use in-session context to augment a generic ranker optimized for conversion and relevance. We use a combination of latent features learned from item co-clicks in historic sessions and content based features that use item title and price. Personalization in search has been discussed extensively in the existing literature. The novelty of our work is combining and comparing content based and content agnostic features and showing that they complement each other to result in a significant improvement of the ranker. We experimentally show that our technique significantly outperforms a generic ranker in terms of Mean Reciprocal Rank (MRR). We also provide anecdotal evidence for the semantic similarity captured by the item embeddings on the eBay search engine.", "title": "Personalized Ranking in eCommerce Search", "authors": ["Grigor Aslanyan", "Aritra Mandal", "Prathyusha Senthil Kumar", "Amit Jaiswal", "Manojkumar Rangasamy Kannadasan"], "year": "2020", "booktitle": "Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020"}, "2020.wwwconf_conference-2020.154": {"doc_id": "2020.wwwconf_conference-2020.154", "default_text": "MadDroid: Characterizing and Detecting Devious Ad Contents for Android Apps ", "abstract": "", "title": "MadDroid: Characterizing and Detecting Devious Ad Contents for Android Apps", "authors": ["Tianming Liu", "Haoyu Wang", "Li Li", "Xiapu Luo", "Feng Dong", "Yao Guo", "Liu Wang", "Tegawend\u00e9 F. Bissyand\u00e9", "Jacques Klein"], "year": "2020", "booktitle": "WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020"}, "2004.wwwconf_conference-2004.1": {"doc_id": "2004.wwwconf_conference-2004.1", "default_text": "What's new on the web?: the evolution of the web from a search engine perspective ABSTRACTWe seek to gain improved insight into how Web search engines should cope with the evolving Web, in an attempt to provide users with the most up-to-date results possible. For this purpose we collected weekly snapshots of some 150 Web sites over the course of one year, and measured the evolution of content and link structure. Our measurements focus on aspects of potential interest to search engine designers: the evolution of link structure over time, the rate of creation of new pages and new distinct content on the Web, and the rate of change of the content of existing pages under search-centric measures of degree of change.Our findings indicate a rapid turnover rate of Web pages, i.e., high rates of birth and death, coupled with an even higher rate of turnover in the hyperlinks that connect them. For pages that persist over time we found that, perhaps surprisingly, the degree of content shift as measured using TF.IDF cosine distance does not appear to be consistently correlated with the frequency of content updating. Despite this apparent noncorrelation, the rate of content shift of a given page is likely to remain consistent over time. That is, pages that change a great deal in one week will likely change by a similarly large degree in the following week. Conversely, pages that experience little change will continue to experience little change. We conclude the paper with a discussion of the potential implications of our results for the design of effective Web search engines.", "abstract": "ABSTRACTWe seek to gain improved insight into how Web search engines should cope with the evolving Web, in an attempt to provide users with the most up-to-date results possible. For this purpose we collected weekly snapshots of some 150 Web sites over the course of one year, and measured the evolution of content and link structure. Our measurements focus on aspects of potential interest to search engine designers: the evolution of link structure over time, the rate of creation of new pages and new distinct content on the Web, and the rate of change of the content of existing pages under search-centric measures of degree of change.Our findings indicate a rapid turnover rate of Web pages, i.e., high rates of birth and death, coupled with an even higher rate of turnover in the hyperlinks that connect them. For pages that persist over time we found that, perhaps surprisingly, the degree of content shift as measured using TF.IDF cosine distance does not appear to be consistently correlated with the frequency of content updating. Despite this apparent noncorrelation, the rate of content shift of a given page is likely to remain consistent over time. That is, pages that change a great deal in one week will likely change by a similarly large degree in the following week. Conversely, pages that experience little change will continue to experience little change. We conclude the paper with a discussion of the potential implications of our results for the design of effective Web search engines.", "title": "What's new on the web?: the evolution of the web from a search engine perspective", "authors": ["Alexandros Ntoulas", "Junghoo Cho", "Christopher Olston"], "year": "2004", "booktitle": "Proceedings of the 13th international conference on World Wide Web, WWW 2004, New York, NY, USA, May 17-20, 2004"}, "2013.wwwconf_conference-2013c.138": {"doc_id": "2013.wwwconf_conference-2013c.138", "default_text": "Effective analysis, characterization, and detection of malicious web pages ABSTRACTThe steady evolution of the Web has paved the way for miscreants to take advantage of vulnerabilities to embed malicious content into web pages. Up on a visit, malicious web pages steal sensitive data, redirect victims to other malicious targets, or cease control of victim's system to mount future attacks. Approaches to detect malicious web pages have been reactively effective at special classes of attacks like drive-by-downloads. However, the prevalence and complexity of attacks by malicious web pages is still worrisome. The main challenges in this problem domain are (1) fine-grained capturing and characterization of attack payloads (2) evolution of web page artifacts and (3) flexibility and scalability of detection techniques with a fast-changing threat landscape. To this end, we proposed a holistic approach that leverages static analysis, dynamic analysis, machine learning, and evolutionary searching and optimization to effectively analyze and detect malicious web pages. We do so by: introducing novel features to capture fine-grained snapshot of malicious web pages, holistic characterization of malicious web pages, and application of evolutionary techniques to finetune learning-based detection models pertinent to evolution of attack payloads. In this paper, we present key intuition and details of our approach, results obtained so far, and future work.", "abstract": "ABSTRACTThe steady evolution of the Web has paved the way for miscreants to take advantage of vulnerabilities to embed malicious content into web pages. Up on a visit, malicious web pages steal sensitive data, redirect victims to other malicious targets, or cease control of victim's system to mount future attacks. Approaches to detect malicious web pages have been reactively effective at special classes of attacks like drive-by-downloads. However, the prevalence and complexity of attacks by malicious web pages is still worrisome. The main challenges in this problem domain are (1) fine-grained capturing and characterization of attack payloads (2) evolution of web page artifacts and (3) flexibility and scalability of detection techniques with a fast-changing threat landscape. To this end, we proposed a holistic approach that leverages static analysis, dynamic analysis, machine learning, and evolutionary searching and optimization to effectively analyze and detect malicious web pages. We do so by: introducing novel features to capture fine-grained snapshot of malicious web pages, holistic characterization of malicious web pages, and application of evolutionary techniques to finetune learning-based detection models pertinent to evolution of attack payloads. In this paper, we present key intuition and details of our approach, results obtained so far, and future work.", "title": "Effective analysis, characterization, and detection of malicious web pages", "authors": ["Birhanu Eshete"], "year": "2013", "booktitle": "22nd International World Wide Web Conference, WWW '13, Rio de Janeiro, Brazil, May 13-17, 2013, Companion Volume"}, "2013.wwwconf_conference-2013.18": {"doc_id": "2013.wwwconf_conference-2013.18", "default_text": "Is this app safe for children?: a comparison study of maturity ratings on Android and iOS applications ABSTRACTThere is a rising concern among parents who have experienced unreliable content maturity ratings for mobile applications (apps) that result in inappropriate risk exposure for their children and adolescents. In reality, there is no consistent maturity rating policy for mobile applications. The maturity ratings of Android apps are provided purely by developers' self-disclosure and are rarely verified. While Apple's iOS app ratings are considered to be more accurate, they can also be inconsistent with Apple's published policies. To address these issues, this research aims to systematically uncover the extent and severity of unreliable maturity ratings for mobile apps. Specifically, we develop mechanisms to verify the maturity ratings of mobile apps and investigate possible reasons behind the incorrect ratings. We believe that our findings have important implications for platform providers (e.g., Google or Apple) as well as for regulatory bodies and application developers.", "abstract": "ABSTRACTThere is a rising concern among parents who have experienced unreliable content maturity ratings for mobile applications (apps) that result in inappropriate risk exposure for their children and adolescents. In reality, there is no consistent maturity rating policy for mobile applications. The maturity ratings of Android apps are provided purely by developers' self-disclosure and are rarely verified. While Apple's iOS app ratings are considered to be more accurate, they can also be inconsistent with Apple's published policies. To address these issues, this research aims to systematically uncover the extent and severity of unreliable maturity ratings for mobile apps. Specifically, we develop mechanisms to verify the maturity ratings of mobile apps and investigate possible reasons behind the incorrect ratings. We believe that our findings have important implications for platform providers (e.g., Google or Apple) as well as for regulatory bodies and application developers.", "title": "Is this app safe for children?: a comparison study of maturity ratings on Android and iOS applications", "authors": ["Ying Chen", "Heng Xu", "Yilu Zhou", "Sencun Zhu"], "year": "2013", "booktitle": "22nd International World Wide Web Conference, WWW '13, Rio de Janeiro, Brazil, May 13-17, 2013"}, "2018.wwwconf_conference-2018.17": {"doc_id": "2018.wwwconf_conference-2018.17", "default_text": "Sharing Deep Neural Network Models with Interpretation ABSTRACTDespite outperforming humans in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, where the model is developed by a third party. For a supervised machine learning model, sharing training process including training data is a way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight into a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated deep neural network models with high fidelity. We show that data point pairs in the tree give users significantly better understanding of the model decision boundaries and paves the way for trustworthy model sharing.", "abstract": "ABSTRACTDespite outperforming humans in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, where the model is developed by a third party. For a supervised machine learning model, sharing training process including training data is a way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight into a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated deep neural network models with high fidelity. We show that data point pairs in the tree give users significantly better understanding of the model decision boundaries and paves the way for trustworthy model sharing.", "title": "Sharing Deep Neural Network Models with Interpretation", "authors": ["Huijun Wu", "Chen Wang", "Jie Yin", "Kai Lu", "Liming Zhu"], "year": "2018", "booktitle": "Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018, Lyon, France, April 23-27, 2018"}, "2018.wwwconf_conference-2018.144": {"doc_id": "2018.wwwconf_conference-2018.144", "default_text": "Aladdin: Automating Release of Deep-Link APIs on Android ABSTRACTCompared to the Web where each web page has a global URL for external access, a specific \"page\" inside a mobile app cannot be easily accessed unless the user performs several steps from the landing page of this app. Recently, the concept of \"deep link\" is expected to be a promising solution and has been advocated by major service providers to enable targeting and opening a specific page of an app externally with an accessible uniform resource identifier. In this paper, we present a large-scale empirical study to investigate how deep links are really adopted, over 25,000 Android apps. To our surprise, we find that deep links have quite low coverage, e.g., more than 70% and 90% of the apps do not have deep links on app stores Wandoujia and Google Play, respectively. One underlying reason is the mandatory and non-trivial manual efforts of app developers to provide APIs for deep links. We then propose the Aladdin approach along with its supporting tool to help developers practically automate the release of deep-link APIs to access locations inside their apps. Aladdin includes a novel cooperative framework by synthesizing the static analysis and the dynamic analysis while minimally engaging developers' inputs and configurations, without requiring any coding efforts or additional deployment efforts. We evaluate Aladdin with 579 popular apps and demonstrate its effectiveness and performance.\nCCS CONCEPTS\u2022 Human-centered computing \u2192 Ubiquitous and mobile computing systems and tools; KEYWORDS Deep link; Android apps; program analysis ACM Reference Format:", "abstract": "ABSTRACTCompared to the Web where each web page has a global URL for external access, a specific \"page\" inside a mobile app cannot be easily accessed unless the user performs several steps from the landing page of this app. Recently, the concept of \"deep link\" is expected to be a promising solution and has been advocated by major service providers to enable targeting and opening a specific page of an app externally with an accessible uniform resource identifier. In this paper, we present a large-scale empirical study to investigate how deep links are really adopted, over 25,000 Android apps. To our surprise, we find that deep links have quite low coverage, e.g., more than 70% and 90% of the apps do not have deep links on app stores Wandoujia and Google Play, respectively. One underlying reason is the mandatory and non-trivial manual efforts of app developers to provide APIs for deep links. We then propose the Aladdin approach along with its supporting tool to help developers practically automate the release of deep-link APIs to access locations inside their apps. Aladdin includes a novel cooperative framework by synthesizing the static analysis and the dynamic analysis while minimally engaging developers' inputs and configurations, without requiring any coding efforts or additional deployment efforts. We evaluate Aladdin with 579 popular apps and demonstrate its effectiveness and performance.\nCCS CONCEPTS\u2022 Human-centered computing \u2192 Ubiquitous and mobile computing systems and tools; KEYWORDS Deep link; Android apps; program analysis ACM Reference Format:", "title": "Aladdin: Automating Release of Deep-Link APIs on Android", "authors": ["Yun Ma", "Ziniu Hu", "Yunxin Liu", "Tao Xie", "Xuanzhe Liu"], "year": "2018", "booktitle": "Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018, Lyon, France, April 23-27, 2018"}, "2018.wwwconf_conference-2018c.377": {"doc_id": "2018.wwwconf_conference-2018c.377", "default_text": "Aspect-based Financial Sentiment Analysis with Deep Neural Networks ABSTRACTAspect-based financial sentiment analysis, which aims to classify the text instance into a pre-defined aspect class and predict the sentiment score for the mentioned target. In this paper, we propose a neural network model, Attention-based LSTM model with the Aspect information (ALA), to solve the financial opinion mining problem introduced by the WWW 2018 shared task. The proposed neural network model can adapt to the financial dataset so that the neural network can effectively understand the semantic information of the short text. We evaluate our model with the 10-fold crossvalidation, and we compare our model with a variety of related deep neural network models.", "abstract": "ABSTRACTAspect-based financial sentiment analysis, which aims to classify the text instance into a pre-defined aspect class and predict the sentiment score for the mentioned target. In this paper, we propose a neural network model, Attention-based LSTM model with the Aspect information (ALA), to solve the financial opinion mining problem introduced by the WWW 2018 shared task. The proposed neural network model can adapt to the financial dataset so that the neural network can effectively understand the semantic information of the short text. We evaluate our model with the 10-fold crossvalidation, and we compare our model with a variety of related deep neural network models.", "title": "Aspect-based Financial Sentiment Analysis with Deep Neural Networks", "authors": ["E. Shijia", "Li Yang", "Mohan Zhang", "Yang Xiang"], "year": "2018", "booktitle": "Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018"}, "2017.wwwconf_conference-2017.18": {"doc_id": "2017.wwwconf_conference-2017.18", "default_text": "AppHolmes: Detecting and Characterizing App Collusion among Third-Party Android Markets ABSTRACTBackground activities on smartphones are essential to today's \"always-on\" mobile device experience. Yet, there lacks a clear understanding of the cooperative behaviors among background activities as well as a quantification of the consequences. In this paper, we present the first in-depth study of app collusion, in which one app surreptitiously launches others in the background without user's awareness. To enable the study, we develop AppHolmes, a static analysis tool for detecting app collusion by examining the app binaries. By analyzing 10,000 apps from top third-party app markets, we found that i) covert, cooperative behaviors in background app launch are surprisingly pervasive, ii) most collusion is caused by shared services, libraries, or common interest among apps, and iii) collusion has serious impact on performance, efficiency, and security. Overall, our work presents a strong implication on future mobile system design.", "abstract": "ABSTRACTBackground activities on smartphones are essential to today's \"always-on\" mobile device experience. Yet, there lacks a clear understanding of the cooperative behaviors among background activities as well as a quantification of the consequences. In this paper, we present the first in-depth study of app collusion, in which one app surreptitiously launches others in the background without user's awareness. To enable the study, we develop AppHolmes, a static analysis tool for detecting app collusion by examining the app binaries. By analyzing 10,000 apps from top third-party app markets, we found that i) covert, cooperative behaviors in background app launch are surprisingly pervasive, ii) most collusion is caused by shared services, libraries, or common interest among apps, and iii) collusion has serious impact on performance, efficiency, and security. Overall, our work presents a strong implication on future mobile system design.", "title": "AppHolmes: Detecting and Characterizing App Collusion among Third-Party Android Markets", "authors": ["Mengwei Xu", "Yun Ma", "Xuanzhe Liu", "Felix Xiaozhu Lin", "Yunxin Liu"], "year": "2017", "booktitle": "Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017"}, "2017.wwwconf_conference-2017.20": {"doc_id": "2017.wwwconf_conference-2017.20", "default_text": "An Explorative Study of the Mobile App Ecosystem from App Developers' Perspective ABSTRACTWith the prevalence of smartphones, app markets such as Apple App Store and Google Play has become the center stage in the mobile app ecosystem, with millions of apps developed by tens of thousands of app developers in each major market. This paper presents a study of the mobile app ecosystem from the perspective of app developers. Based on over one million Android apps and 320,000 developers from Google Play, we analyzed the Android app ecosystem from different aspects. Our analysis shows that while over half of the developers have released only one app in the market, many of them have released hundreds of apps. We classified developers into different groups based on the number of apps they have released, and compared their characteristics. Specially, we have analyzed the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps. We also investigated the privacy behaviors of app developers, showing that some developers have a habit of producing apps with low privacy ratings. Our study shows that understanding the behavior of mobile developers can be helpful to not only other app developers, but also to app markets and mobile users.", "abstract": "ABSTRACTWith the prevalence of smartphones, app markets such as Apple App Store and Google Play has become the center stage in the mobile app ecosystem, with millions of apps developed by tens of thousands of app developers in each major market. This paper presents a study of the mobile app ecosystem from the perspective of app developers. Based on over one million Android apps and 320,000 developers from Google Play, we analyzed the Android app ecosystem from different aspects. Our analysis shows that while over half of the developers have released only one app in the market, many of them have released hundreds of apps. We classified developers into different groups based on the number of apps they have released, and compared their characteristics. Specially, we have analyzed the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps. We also investigated the privacy behaviors of app developers, showing that some developers have a habit of producing apps with low privacy ratings. Our study shows that understanding the behavior of mobile developers can be helpful to not only other app developers, but also to app markets and mobile users.", "title": "An Explorative Study of the Mobile App Ecosystem from App Developers' Perspective", "authors": ["Haoyu Wang", "Zhe Liu", "Yao Guo", "Xiangqun Chen", "Miao Zhang", "Guoai Xu", "Jason I. Hong"], "year": "2017", "booktitle": "Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017"}, "2003.wwwconf_conference-2003.70": {"doc_id": "2003.wwwconf_conference-2003.70", "default_text": "A large-scale study of the evolution of web pages ABSTRACTHow fast does the web change? Does most of the content remain unchanged once it has been authored, or are the documents continuously updated? Do pages change a little or a lot? Is the extent of change correlated to any other property of the page? All of these questions are of interest to those who mine the web, including all the popular search engines, but few studies have been performed to date to answer them.One notable exception is a study by Cho and Garcia-Molina, who crawled a set of 720,000 pages on a daily basis over four months, and counted pages as having changed if their MD5 checksum changed. They found that 40% of all web pages in their set changed within a week, and 23% of those pages that fell into the .com domain changed daily.This paper expands on Cho and Garcia-Molina's study, both in terms of coverage and in terms of sensitivity to change. We crawled a set of 150,836,209 HTML pages once every week, over a span of 11 weeks. For each page, we recorded a checksum of the page, and a feature vector of the words on the page, plus various other data such as the page length, the HTTP status code, etc. Moreover, we pseudo-randomly selected 0.1% of all of our URLs, and saved the full text of each download of the corresponding pages.After completion of the crawl, we analyzed the degree of change of each page, and investigated which factors are correlated with change intensity. We found that the average degree of change varies widely across top-level domains, and that larger pages change more often and more severely than smaller ones. This paper describes the crawl and the data transformations we performed on the logs, and presents some statistical observations on the degree of change of different classes of pages.", "abstract": "ABSTRACTHow fast does the web change? Does most of the content remain unchanged once it has been authored, or are the documents continuously updated? Do pages change a little or a lot? Is the extent of change correlated to any other property of the page? All of these questions are of interest to those who mine the web, including all the popular search engines, but few studies have been performed to date to answer them.One notable exception is a study by Cho and Garcia-Molina, who crawled a set of 720,000 pages on a daily basis over four months, and counted pages as having changed if their MD5 checksum changed. They found that 40% of all web pages in their set changed within a week, and 23% of those pages that fell into the .com domain changed daily.This paper expands on Cho and Garcia-Molina's study, both in terms of coverage and in terms of sensitivity to change. We crawled a set of 150,836,209 HTML pages once every week, over a span of 11 weeks. For each page, we recorded a checksum of the page, and a feature vector of the words on the page, plus various other data such as the page length, the HTTP status code, etc. Moreover, we pseudo-randomly selected 0.1% of all of our URLs, and saved the full text of each download of the corresponding pages.After completion of the crawl, we analyzed the degree of change of each page, and investigated which factors are correlated with change intensity. We found that the average degree of change varies widely across top-level domains, and that larger pages change more often and more severely than smaller ones. This paper describes the crawl and the data transformations we performed on the logs, and presents some statistical observations on the degree of change of different classes of pages.", "title": "A large-scale study of the evolution of web pages", "authors": ["Dennis Fetterly", "Mark S. Manasse", "Marc Najork", "Janet L. Wiener"], "year": "2003", "booktitle": "Proceedings of the Twelfth International World Wide Web Conference, WWW 2003, Budapest, Hungary, May 20-24, 2003"}, "2005.wwwconf_conference-2005si.15": {"doc_id": "2005.wwwconf_conference-2005si.15", "default_text": "The volume and evolution of web page templates ABSTRACTWeb pages contain a combination of unique content and template material, which is present across multiple pages and used primarily for formatting, navigation, and branding. We study the nature, evolution, and prevalence of these templates on the web. As part of this work, we develop new randomized algorithms for template extraction that perform approximately twenty times faster than existing approaches with similar quality. Our results show that 40-50% of the content on the web is template content. Over the last eight years, the fraction of template content has doubled, and the growth shows no sign of abating. Text, links, and total HTML bytes within templates are all growing as a fraction of total content at a rate of between 6 and 8% per year. We discuss the deleterious implications of this growth for information retrieval and ranking, classification, and link analysis.", "abstract": "ABSTRACTWeb pages contain a combination of unique content and template material, which is present across multiple pages and used primarily for formatting, navigation, and branding. We study the nature, evolution, and prevalence of these templates on the web. As part of this work, we develop new randomized algorithms for template extraction that perform approximately twenty times faster than existing approaches with similar quality. Our results show that 40-50% of the content on the web is template content. Over the last eight years, the fraction of template content has doubled, and the growth shows no sign of abating. Text, links, and total HTML bytes within templates are all growing as a fraction of total content at a rate of between 6 and 8% per year. We discuss the deleterious implications of this growth for information retrieval and ranking, classification, and link analysis.", "title": "The volume and evolution of web page templates", "authors": ["David Gibson", "Kunal Punera", "Andrew Tomkins"], "year": "2005", "booktitle": "Proceedings of the 14th international conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 - Special interest tracks and posters"}, "2011.trec_conference-2011.102": {"doc_id": "2011.trec_conference-2011.102", "default_text": "The University of Amsterdam at the TREC 2011 Session Track We describe the participation of the University of Amsterdam's ILPS group in the Session track at TREC 2011.", "abstract": "We describe the participation of the University of Amsterdam's ILPS group in the Session track at TREC 2011.", "title": "The University of Amsterdam at the TREC 2011 Session Track", "authors": ["Bouke Huurnink", "Richard Berendsen", "Katja Hofmann", "Edgar Meij", "Maarten de Rijke"], "year": "2011", "booktitle": "Proceedings of The Twentieth Text REtrieval Conference, TREC 2011, Gaithersburg, Maryland, USA, November 15-18, 2011"}, "2015.trec_conference-2015.70": {"doc_id": "2015.trec_conference-2015.70", "default_text": "The University of Amsterdam (ILPS) at TREC 2015 Total Recall Track We describe the participation of the University of Amsterdams ILPS group in the Total Recall track at TREC 2015. Based on the provided Baseline Model Implemention (\"BMI\") we set out to provide two more baselines we can compare to in future work. The two methods are bootstrapped by a synthetic document based on the query, use TF/IDF features, and sample with dynamic batch sizes which depend on the percentage of predicted relevant documents. We sample at least 1 percent of the corpus and stop sampling if a batch contains no relevant documents. The methods differ in the classifier used, i.e. Logistic Regression and Random Forest.", "abstract": "We describe the participation of the University of Amsterdams ILPS group in the Total Recall track at TREC 2015. Based on the provided Baseline Model Implemention (\"BMI\") we set out to provide two more baselines we can compare to in future work. The two methods are bootstrapped by a synthetic document based on the query, use TF/IDF features, and sample with dynamic batch sizes which depend on the percentage of predicted relevant documents. We sample at least 1 percent of the corpus and stop sampling if a batch contains no relevant documents. The methods differ in the classifier used, i.e. Logistic Regression and Random Forest.", "title": "The University of Amsterdam (ILPS) at TREC 2015 Total Recall Track", "authors": ["David van Dijk", "Zhaochun Ren", "Evangelos Kanoulas", "Maarten de Rijke"], "year": "2015", "booktitle": "Proceedings of The Twenty-Fourth Text REtrieval Conference, TREC 2015, Gaithersburg, Maryland, USA, November 17-20, 2015"}, "2012.trec_conference-2012.73": {"doc_id": "2012.trec_conference-2012.73", "default_text": "The University of Amsterdam at TREC 2012 We describe the participation of the University of Amsterdam's ILPS group in the Knowledge Base Acceleration and Microblog tracks at TREC 2012.", "abstract": "We describe the participation of the University of Amsterdam's ILPS group in the Knowledge Base Acceleration and Microblog tracks at TREC 2012.", "title": "The University of Amsterdam at TREC 2012", "authors": ["Richard Berendsen", "Edgar Meij", "Daan Odijk", "Maarten de Rijke", "Wouter Weerkamp"], "year": "2012", "booktitle": "Proceedings of The Twenty-First Text REtrieval Conference, TREC 2012, Gaithersburg, Maryland, USA, November 6-9, 2012"}, "2018.trec_conference-2018.8": {"doc_id": "2018.trec_conference-2018.8", "default_text": "Exploring post retrieval techniques for bio-medical domain retrieval Retrieval is a common way to access the huge data collection in bio-medical domain. For instance, the physicians would need to retrieve relevant documents to treat cancer for the patients. With the huge number of documents in the collection, a reliable retrieval system is critical for a satisfying performance. This year's TREC Precision Medicine track continues the same procedure as last year by providing us the platform for testing different methods for biomedical retrieval. For this year, we used similar methods as we used in TREC PM17, i.e., term based representation and concept based representation. In addition, we also tried to combine the different representation with results filtering and two-round retrieval. The results show that the modification on the methods, i.e., results filtering and two round retrieval, did not outperform the baseline methods.", "abstract": "Retrieval is a common way to access the huge data collection in bio-medical domain. For instance, the physicians would need to retrieve relevant documents to treat cancer for the patients. With the huge number of documents in the collection, a reliable retrieval system is critical for a satisfying performance. This year's TREC Precision Medicine track continues the same procedure as last year by providing us the platform for testing different methods for biomedical retrieval. For this year, we used similar methods as we used in TREC PM17, i.e., term based representation and concept based representation. In addition, we also tried to combine the different representation with results filtering and two-round retrieval. The results show that the modification on the methods, i.e., results filtering and two round retrieval, did not outperform the baseline methods.", "title": "Exploring post retrieval techniques for bio-medical domain retrieval", "authors": ["Yue Wang", "Hui Fang"], "year": "2018", "booktitle": "Proceedings of the Twenty-Seventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, November 14-16, 2018"}, "2012.tist_journal-ir0anthology0volumeA3A4.10": {"doc_id": "2012.tist_journal-ir0anthology0volumeA3A4.10", "default_text": "Information Retrieval in the Commentsphere This article studies information retrieval tasks related to Web comments. Prerequisite of such a study and a main contribution of the article is a unifying survey of the research field. We identify the most important retrieval tasks related to comments, namely filtering, ranking, and summarization. Within these tasks, we distinguish two paradigms according to which comments are utilized and which we designate as commenttargeting and comment-exploiting. Within the first paradigm, the comments themselves form the retrieval targets. Within the second paradigm, the commented items form the retrieval targets (i.e., comments are used as an additional information source to improve the retrieval performance for the commented items). We report on four case studies to demonstrate the exploration of the commentsphere under information retrieval aspects: comment filtering, comment ranking, comment summarization and cross-media retrieval. The first three studies deal primarily with comment-targeting retrieval, while the last one deals with commentexploiting retrieval. Throughout the article, connections to information retrieval research are pointed out.", "abstract": "This article studies information retrieval tasks related to Web comments. Prerequisite of such a study and a main contribution of the article is a unifying survey of the research field. We identify the most important retrieval tasks related to comments, namely filtering, ranking, and summarization. Within these tasks, we distinguish two paradigms according to which comments are utilized and which we designate as commenttargeting and comment-exploiting. Within the first paradigm, the comments themselves form the retrieval targets. Within the second paradigm, the commented items form the retrieval targets (i.e., comments are used as an additional information source to improve the retrieval performance for the commented items). We report on four case studies to demonstrate the exploration of the commentsphere under information retrieval aspects: comment filtering, comment ranking, comment summarization and cross-media retrieval. The first three studies deal primarily with comment-targeting retrieval, while the last one deals with commentexploiting retrieval. Throughout the article, connections to information retrieval research are pointed out.", "title": "Information Retrieval in the Commentsphere", "authors": ["Martin Potthast", "Benno Stein", "Fabian Loose", "Steffen Becker"], "year": "2012", "booktitle": "2012 Volume 3 Issue 4"}, "2018.wwwjournals_journal-ir0anthology0volumeA21A1.5": {"doc_id": "2018.wwwjournals_journal-ir0anthology0volumeA21A1.5", "default_text": "Discovering and understanding android sensor usage behaviors with data flow analysis Abstract Today's Android-powered smartphones have various embedded sensors that measure the acceleration, orientation, light and other environmental conditions. Many functions in the third-party applications (apps) need to use these sensors. However, embedded sensors may lead to security issues, as the third-party apps can read data from these sensors without claiming any permissions. It has been proven that embedded sensors can be exploited by well designed malicious apps, resulting in leaking users' privacy. In this work, we are motivated to provide an overview of sensor usage patterns in current apps by investigating what, why and how embedded sensors are used in the apps collected from both a Chinese app market called \"AppChina\" and the official market called \"Google Play\". To fulfill this goal, We develop a tool called \"SDFDroid\" to identify the used sensors' types and to generate the sensor data propagation graphs in each app. We then cluster the apps to find out their sensor usage patterns based on their sensor data propagation graphs. We apply our method on 22,010 apps collected from AppChina and 7,601 apps from Google Play. Extensive experiments are conducted and the experimental results show that most apps implement their sensor related functions by using the third-party libraries. We further study the sensor usage behaviors in the third-party libraries. Our results show that the accelerometer is the most frequently used sensor. Though many third-party libraries use no more than four types of sensors, there are still some third-party libraries registering all the types of sensors recklessly. These results call for more attentions on better regulating the sensor usage in Android apps.", "abstract": "Abstract Today's Android-powered smartphones have various embedded sensors that measure the acceleration, orientation, light and other environmental conditions. Many functions in the third-party applications (apps) need to use these sensors. However, embedded sensors may lead to security issues, as the third-party apps can read data from these sensors without claiming any permissions. It has been proven that embedded sensors can be exploited by well designed malicious apps, resulting in leaking users' privacy. In this work, we are motivated to provide an overview of sensor usage patterns in current apps by investigating what, why and how embedded sensors are used in the apps collected from both a Chinese app market called \"AppChina\" and the official market called \"Google Play\". To fulfill this goal, We develop a tool called \"SDFDroid\" to identify the used sensors' types and to generate the sensor data propagation graphs in each app. We then cluster the apps to find out their sensor usage patterns based on their sensor data propagation graphs. We apply our method on 22,010 apps collected from AppChina and 7,601 apps from Google Play. Extensive experiments are conducted and the experimental results show that most apps implement their sensor related functions by using the third-party libraries. We further study the sensor usage behaviors in the third-party libraries. Our results show that the accelerometer is the most frequently used sensor. Though many third-party libraries use no more than four types of sensors, there are still some third-party libraries registering all the types of sensors recklessly. These results call for more attentions on better regulating the sensor usage in Android apps.", "title": "Discovering and understanding android sensor usage behaviors with data flow analysis", "authors": ["Xing Liu", "Jiqiang Liu", "Wei Wang", "Yongzhong He", "Xiangliang Zhang"], "year": "2018", "booktitle": "2018 Volume 21 Issue 1"}, "2018.wwwjournals_journal-ir0anthology0volumeA21A1.6": {"doc_id": "2018.wwwjournals_journal-ir0anthology0volumeA21A1.6", "default_text": "An automatically vetting mechanism for SSL error-handling vulnerability in android hybrid Web apps Abstract A large set of diverse hybrid mobile apps, which use both native Android app UIs and Web UIs, are widely available in today's smartphones. These hybrid apps usually use SSL or TLS to secure HTTP based communication. However, researchers show that incorrect implementation of SSL or TLS may lead to serious security problems, such as Man-In-The-Middle (MITM) attacks and phishing attacks. This paper investigates a particular SSL vulnerability that results from error-handling code in the hybrid mobile Web apps. Usually such error-handling code is used to terminate an ongoing communication, but the vulnerability of interest is able to make the communication proceed regardless of SSL certificate verification failures, eventually lead to MITM attacks. To identify those vulnerable apps, we develop a hybrid approach, which combines both static analysis and dynamic analysis to   Web UIs to trigger the error-handling code; (2) accurately select the correct paths from the app entry-point to the targeted code, meanwhile avoiding the crash of apps, and populate messaging objects for the communication between components. Specifically, we construct inter-component call graphs to model the connections, and design algorithms to select the paths from the established graph and determine the parameters by backtracing. To evaluate our approach, we have implemented and tested it with 13,820 real world mobile Web apps from Google Play. The experimental results demonstrate that 1,360 apps are detected as potentially vulnerable ones solely using the static analysis. The dynamic analysis process further confirms that 711 apps are truly vulnerable among the potentially vulnerable set.", "abstract": "Abstract A large set of diverse hybrid mobile apps, which use both native Android app UIs and Web UIs, are widely available in today's smartphones. These hybrid apps usually use SSL or TLS to secure HTTP based communication. However, researchers show that incorrect implementation of SSL or TLS may lead to serious security problems, such as Man-In-The-Middle (MITM) attacks and phishing attacks. This paper investigates a particular SSL vulnerability that results from error-handling code in the hybrid mobile Web apps. Usually such error-handling code is used to terminate an ongoing communication, but the vulnerability of interest is able to make the communication proceed regardless of SSL certificate verification failures, eventually lead to MITM attacks. To identify those vulnerable apps, we develop a hybrid approach, which combines both static analysis and dynamic analysis to   Web UIs to trigger the error-handling code; (2) accurately select the correct paths from the app entry-point to the targeted code, meanwhile avoiding the crash of apps, and populate messaging objects for the communication between components. Specifically, we construct inter-component call graphs to model the connections, and design algorithms to select the paths from the established graph and determine the parameters by backtracing. To evaluate our approach, we have implemented and tested it with 13,820 real world mobile Web apps from Google Play. The experimental results demonstrate that 1,360 apps are detected as potentially vulnerable ones solely using the static analysis. The dynamic analysis process further confirms that 711 apps are truly vulnerable among the potentially vulnerable set.", "title": "An automatically vetting mechanism for SSL error-handling vulnerability in android hybrid Web apps", "authors": ["Yang Liu", "Chaoshun Zuo", "Zonghua Zhang", "Shanqing Guo", "Xin-Shun Xu"], "year": "2018", "booktitle": "2018 Volume 21 Issue 1"}, "2020.tweb_journal-ir0anthology0volumeA14A3.4": {"doc_id": "2020.tweb_journal-ir0anthology0volumeA14A3.4", "default_text": "Roaming Through the Castle Tunnels: An Empirical Analysis of Inter-app Navigation of Android Apps ", "abstract": "", "title": "Roaming Through the Castle Tunnels: An Empirical Analysis of Inter-app Navigation of Android Apps", "authors": ["Yun Ma", "Ziniu Hu", "Diandian Gu", "Li Zhou", "Qiaozhu Mei", "Gang Huang", "Xuanzhe Liu"], "year": "2020", "booktitle": "2020 Volume 14 Issue 3"}, "2013.sigirjournals_journal-ir0anthology0volumeA47A1.11": {"doc_id": "2013.sigirjournals_journal-ir0anthology0volumeA47A1.11", "default_text": "Sub-document level information retrieval: retrieval and evaluation AbstractXML is increasingly used to mark up content in present day information repositories. Over the last decade or so, retrieval from XML document collections has emerged as an area of active research. For the Information Retrieval community, XML retrieval poses a two-fold problem:1. finding effective techniques to retrieve appropriate or the most useful XML elements in response to a user query; and 2. devising an appropriate evaluation methodology to measure the effectivity of such retrieval techniques.This study examines both these issues. First, we revisited the pivoted length normalization scheme in the Vector Space Model using standard benchmark collections for XML retrieval. We reduced two parameters used in pivoted length normalization to a single combined parameter and experimentally found its optimum value, which works well at both the element and document levels for XML retrieval.We observed that a substantial number of focused queries used in XML retrieval clearly state, besides the information need, what the user does not want. We demonstrated that this negative information, if not handled properly, degrades retrieval performance. We proposed a solution for automatically removing negative information from XML queries. This led to significant improvements in retrieval results.On the evaluation of XML retrieval, we first studied the sensitivity & robustness of various evaluation metrics and reliability and reusability of the assessment pool that has been used at INEX Ad Hoc track since 2007. Specifically we investigated the behaviour of the metrics when assessments are incomplete, or when query sets are small. We observed that early precision metrics are more error-prone and less stable under both these conditions. Average metric, however, performs comparatively better in this respect. System rankings remain largely unaffected even when assessment effort is substantially (but systematically) reduced. We also found that the INEX collections remain usable when evaluating non-participating systems.", "abstract": "AbstractXML is increasingly used to mark up content in present day information repositories. Over the last decade or so, retrieval from XML document collections has emerged as an area of active research. For the Information Retrieval community, XML retrieval poses a two-fold problem:1. finding effective techniques to retrieve appropriate or the most useful XML elements in response to a user query; and 2. devising an appropriate evaluation methodology to measure the effectivity of such retrieval techniques.This study examines both these issues. First, we revisited the pivoted length normalization scheme in the Vector Space Model using standard benchmark collections for XML retrieval. We reduced two parameters used in pivoted length normalization to a single combined parameter and experimentally found its optimum value, which works well at both the element and document levels for XML retrieval.We observed that a substantial number of focused queries used in XML retrieval clearly state, besides the information need, what the user does not want. We demonstrated that this negative information, if not handled properly, degrades retrieval performance. We proposed a solution for automatically removing negative information from XML queries. This led to significant improvements in retrieval results.On the evaluation of XML retrieval, we first studied the sensitivity & robustness of various evaluation metrics and reliability and reusability of the assessment pool that has been used at INEX Ad Hoc track since 2007. Specifically we investigated the behaviour of the metrics when assessments are incomplete, or when query sets are small. We observed that early precision metrics are more error-prone and less stable under both these conditions. Average metric, however, performs comparatively better in this respect. System rankings remain largely unaffected even when assessment effort is substantially (but systematically) reduced. We also found that the INEX collections remain usable when evaluating non-participating systems.", "title": "Sub-document level information retrieval: retrieval and evaluation", "authors": ["Sukomal Pal"], "year": "2013", "booktitle": "2013 Volume 47 Issue 1"}, "2012.sigirjournals_journal-ir0anthology0volumeA46A1.9": {"doc_id": "2012.sigirjournals_journal-ir0anthology0volumeA46A1.9", "default_text": "Evaluating retrieval models using retrievability measurement AbstractEvaluation is the main driving force in research, development and applications related to information retrieval (IR). In the traditional IR evaluation paradigm a list of query topics along with their relevance judgments are given. The main limitation of this kind of evaluation paradigm is that it focuses almost exclusively on a small set of judged documents and does not consider what influence the given retrieval models have on accessing all the relevant information in the collection. This is particularly important for recall oriented retrieval applications where we want to ensure that that everything relevant has been found.In this thesis we analyze the effectiveness of retrieval models from the documents retrievability point of view. We focus particularly on the retrieval bias of different retrieval models, and try to examine to what extent this bias restricts the users in retrieving relevant information. We explore this research with the help of three factors. First, we analyze the relationship between different characteristics of queries and retrievability. This is important from the query generation point of view, since in case of exhaustive queries, it is practically infeasible to complete retrievability approximation in reasonable time. The strong correlation between retrievability and query characteristics allows us to approximate the retrievability score accurately with the help of a query subset without processing an exhaustive number of queries. After this, we examine to what extent the retrievability and other IR effectiveness measures are related to each other. This specifically helps us to understand to what extent it is possible to automatically rank the effectiveness of retrieval models on the basis of their retrieval bias. This also offers a basis for optimizing retrieval systems for specific collections without the need to provide manually annotated ground truth. This is particularly useful for those retrieval domains where it is difficult to obtain a sufficient amount of relevance judgments. At the end we investigate and devise different retrieval strategies for mitigating the effect of low retrievability of documents. These include collection partitioning and query expansion on the basis of improved pseudo relevance feedback selection.The work present in this thesis provides an a novel approach for the evaluation and optimization of retrieval models particularly for recall oriented retrieval domains, where the focus is on retrieving all relevant information but not just retrieving a subset of relevant information. Available online at: http://www.ifs.tuwien.ac.at/~bashir/shariqbashir_ phd_thesis.pdf", "abstract": "AbstractEvaluation is the main driving force in research, development and applications related to information retrieval (IR). In the traditional IR evaluation paradigm a list of query topics along with their relevance judgments are given. The main limitation of this kind of evaluation paradigm is that it focuses almost exclusively on a small set of judged documents and does not consider what influence the given retrieval models have on accessing all the relevant information in the collection. This is particularly important for recall oriented retrieval applications where we want to ensure that that everything relevant has been found.In this thesis we analyze the effectiveness of retrieval models from the documents retrievability point of view. We focus particularly on the retrieval bias of different retrieval models, and try to examine to what extent this bias restricts the users in retrieving relevant information. We explore this research with the help of three factors. First, we analyze the relationship between different characteristics of queries and retrievability. This is important from the query generation point of view, since in case of exhaustive queries, it is practically infeasible to complete retrievability approximation in reasonable time. The strong correlation between retrievability and query characteristics allows us to approximate the retrievability score accurately with the help of a query subset without processing an exhaustive number of queries. After this, we examine to what extent the retrievability and other IR effectiveness measures are related to each other. This specifically helps us to understand to what extent it is possible to automatically rank the effectiveness of retrieval models on the basis of their retrieval bias. This also offers a basis for optimizing retrieval systems for specific collections without the need to provide manually annotated ground truth. This is particularly useful for those retrieval domains where it is difficult to obtain a sufficient amount of relevance judgments. At the end we investigate and devise different retrieval strategies for mitigating the effect of low retrievability of documents. These include collection partitioning and query expansion on the basis of improved pseudo relevance feedback selection.The work present in this thesis provides an a novel approach for the evaluation and optimization of retrieval models particularly for recall oriented retrieval domains, where the focus is on retrieving all relevant information but not just retrieving a subset of relevant information. Available online at: http://www.ifs.tuwien.ac.at/~bashir/shariqbashir_ phd_thesis.pdf", "title": "Evaluating retrieval models using retrievability measurement", "authors": ["Shariq Bashir"], "year": "2012", "booktitle": "2012 Volume 46 Issue 1"}, "2006.ipm_journal-ir0anthology0volumeA42A4.12": {"doc_id": "2006.ipm_journal-ir0anthology0volumeA42A4.12", "default_text": "The effectiveness of Web search engines for retrieving relevant ecommerce links AbstractEcommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines\u00d5 quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.", "abstract": "AbstractEcommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines\u00d5 quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.", "title": "The effectiveness of Web search engines for retrieving relevant ecommerce links", "authors": ["Bernard J. Jansen", "Paulo R. Molina"], "year": "2006", "booktitle": "2006 Volume 42 Issue 4"}, "2005.ipm_journal-ir0anthology0volumeA41A2.1": {"doc_id": "2005.ipm_journal-ir0anthology0volumeA41A2.1", "default_text": "Re-ranking algorithm using post-retrieval clustering for content-based image retrieval AbstractIn this paper, we propose a re-ranking algorithm using post-retrieval clustering for content-based image retrieval (CBIR). In conventional CBIR systems, it is often observed that images visually dissimilar to a query image are ranked high in retrieval results. To remedy this problem, we utilize the similarity relationship of the retrieved results via post-retrieval clustering. In the first step of our method, images are retrieved using visual features such as color histogram. Next, the retrieved images are analyzed using hierarchical agglomerative clustering methods (HACM) and the rank of the results is adjusted according to the distance of a cluster from a query. In addition, we analyze the effects of clustering methods, querycluster similarity functions, and weighting factors in the proposed method. We conducted a number of experiments using several clustering methods and cluster parameters. Experimental results show that the proposed method achieves an improvement of retrieval effectiveness of over 10% on average in the average normalized modified retrieval rank (ANMRR) measure.", "abstract": "AbstractIn this paper, we propose a re-ranking algorithm using post-retrieval clustering for content-based image retrieval (CBIR). In conventional CBIR systems, it is often observed that images visually dissimilar to a query image are ranked high in retrieval results. To remedy this problem, we utilize the similarity relationship of the retrieved results via post-retrieval clustering. In the first step of our method, images are retrieved using visual features such as color histogram. Next, the retrieved images are analyzed using hierarchical agglomerative clustering methods (HACM) and the rank of the results is adjusted according to the distance of a cluster from a query. In addition, we analyze the effects of clustering methods, querycluster similarity functions, and weighting factors in the proposed method. We conducted a number of experiments using several clustering methods and cluster parameters. Experimental results show that the proposed method achieves an improvement of retrieval effectiveness of over 10% on average in the average normalized modified retrieval rank (ANMRR) measure.", "title": "Re-ranking algorithm using post-retrieval clustering for content-based image retrieval", "authors": ["Gunhan Park", "Yunju Baek", "Heung-Kyu Lee"], "year": "2005", "booktitle": "2005 Volume 41 Issue 2"}}};
  </script>
  <script type="text/javascript">
    var allWeightsA = {};
    var allWeightsB = {};
    var mergedWeights = {};
    var COLOR_A = '236, 154, 8';
    var COLOR_B = '121, 196, 121';
    var singleRunView = (data.meta.run2_name === null);

    function markup(text, weights) {
      weights = weights.filter(function (e) {
        return (e[2] > 0 || typeof e[2] === 'string');
      })
      var $result = $('<div></div>');
      if (weights.length === 0) {
        $result.text(text);
      } else {
        $result.append($('<span></span>').text(text.substring(0, weights[0][0])));
        $.each(weights, function (i, weight) {
          if (typeof weight[2] === 'string') {
            var weightColor = weight[2];
          } else {
            var weightColor = 'rgba(255, 237, 140, ' + weight[2].toString() + ')';
          }
          $result.append($('<mark></mark>').text(text.substring(weight[0], weight[1])).css('background', weightColor).attr("run1", weight[3]).attr("run2", weight[4]));
          if (i + 1 < weights.length) {
            $result.append($('<span></span>').text(text.substring(weight[1], weights[i + 1][0])));
          }
        });
        $result.append($('<span></span>').text(text.substring(weights[weights.length - 1][1], text.length)));
      }
      return $result;
    }

    function colorizeWeights(mergedWeights) {
      // deep copu & handle if doesn't exist
      mergedWeights = mergedWeights ? JSON.parse(JSON.stringify(mergedWeights)) : [];
      var results = mergedWeights.map((segment) => {
        if (!("run2" in segment[2]) || segment[2].run2 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')', segment[2].run1, segment[2].run2];
        } else if (!("run" in segment[2]) || segment[2].run1 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')', segment[2].run1, segment[2].run2];
        } else {
          var nil = 'rgba(0, 0, 0, 0)'
          var colorA = 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')';
          var colorB = 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')';
          var overlapColors = 'linear-gradient(' + colorA + ', ' + nil + '), linear-gradient(' + nil + ', ' + colorB + ')'
          return [segment[0], segment[1], overlapColors, segment[2].run1, segment[2].run2];
        }
      })
      return results;
    }
    function generateDocListSingleView(run, container, oneRunWeights) {
      $(container).empty();
      $(container).css("padding-left", "15%").css("padding-right", "15%");
      $.each(run, function (i, doc) {
        oneRunWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text('⋮ ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css("right", '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        var newEl = $('<div></div>')
          .append($('<div class="card"></div>')
            .attr('data-docid', doc.doc_id)
            .attr('run1-rank', doc.rank)
            .append($('<div class="card-header"></div>')
              // .css('padding-' + docIdFloat, '30px')
              .append(doc.rank)
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }
    function generateDocList(run, otherRun, container, docIdFloat, allWeights) {
      $(container).empty();
      $.each(run, function (i, doc) {
        allWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text('⋮ ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css(docIdFloat, '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        // $text.append(' ').append('<a href="#" class="doc-info" role="button">See more</a>');
        var otherRank = null;
        $.each(otherRun, function (i, otherDoc) {
          if (otherDoc.doc_id === doc.doc_id) {
            otherRank = otherDoc.rank;
            return false; // break
          }
        });
        if (otherRank === null) {
          var symbol = '×';
          var tip = 'not ranked in other run';
        }
        else if (doc.rank === otherRank) {
          var symbol = docIdFloat === 'right' ? '→' : '←';
          var tip = 'ranked equally in other run'
        } else if (doc.rank < otherRank) {
          var symbol = docIdFloat === 'right' ? '⬊' : '⬋';
          var tip = 'ranked lower in other run (' + otherRank + ')'
        } else if (doc.rank > otherRank) {
          var symbol = docIdFloat === 'right' ? '⬈' : '⬉';
          var tip = 'ranked higher in other run (' + otherRank + ')'
        }
        var newEl = $('<div></div>')
          // .append($('<span class="other-rank"></span>').text(symbol).css('float', docIdFloat).css('text-align', docIdFloat === 'right' ? 'left' : 'right').attr('title', tip))
          .append($('<div class="card"></div>')
            .attr("run1-rank", docIdFloat === 'right' ? doc.rank : (otherRank === null ? "No" : otherRank))
            .attr("run2-rank", docIdFloat === 'right' ? (otherRank === null ? "No" : otherRank): doc.rank)
            .attr('data-docid', doc.doc_id)
            .append($('<div class="card-header"></div>')
              .css('padding-' + docIdFloat, '30px')
              .append($("<span class='border badge' style='min-width: 50px; font-weight: normal;color: grey;'></span>").html('<span style="font-size: 1.2em;font-weight:bold; color: black;">'+doc.rank +'</span> '+symbol + (otherRank === null ? '': otherRank)).attr('title', tip))
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }

    function selectQuery() {
      var $select = $('#Queries');
      var query_id = $select.val();
      var query = data.queries.filter(query => query.fields.query_id === query_id);
      mergedWeights = query[0].mergedWeights
      var $query = $('#Query');
      $query.empty();
      var $table = $('<table class="fields"></table>').appendTo($query);
      if (query.length > 0) {
        query = query[0];
        $.each(query.fields, function (fname, fvalue) {
          if (fname == "contrast"){
            fvalue = fvalue.name +" (" + fvalue.value.toFixed(3)+")";
            $("#contrast-measure").text("Contrast measure: "+fvalue);
          } else {
            $('<tr></tr>')
            .append($('<th></th>').text(fname))
            .append($('<td></td>').text(fvalue))
            .appendTo($table);
          }
        });
        colors = ["badge-secondary", "badge-info", "badge-warning", "badge-primary"]
        $summary = $("<ul></ul>")
        $.each(query.summary, function(index, data){
          $l=$("<li></li>");
          $.each(data, function(idx, statement){
            $("<span class='badge "+ colors[index] +"'></span>").text(statement).appendTo($l);
          });
          $summary.append($l);
        });
        $("#ranking-summary").empty().append($summary)
        if (singleRunView) {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Value</td>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0]==null? "No" : metric_value[0].toFixed(3)))
            .appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);          
          allWeightsA = {}
          generateDocListSingleView(query.run_1, "#docList", allWeightsA);
        } else {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Run1</td> <td>Run2</td></tr></thead>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0] == null? "No" : metric_value[0].toFixed(3)))
            .append($('<td></td>').text(metric_value[1] == null? "No" : metric_value[1].toFixed(3))).appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);
          allWeightsA = {};
          allWeightsB = {};          
          generateDocList(query.run_1, query.run_2, '#Run1Docs', 'right', allWeightsA);
          generateDocList(query.run_2, query.run_1, '#Run2Docs', 'left', allWeightsB);          
        }
      }
      var extraFields = $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse");
      // Don't show expand/collapse button if there are not fields to expand/collapse
      $('#query-collapse-btn').toggle(extraFields.length > 0);
    }

    function checkThreshold(value, threshold) {
      if (typeof value !== 'undefined') {
        return parseFloat(value) < threshold;
      } else return true;
    }

    function onChangeWeightThreshold() {
      $("#DocumentDetails mark").removeClass("nobackground")
      var run1Threshold = parseFloat($("#run1Threshold").text());
      var run2Threshold = parseFloat($("#run2Threshold").text());
      $("#DocumentDetails mark").each(function () {
        var run1w = $(this).attr("run1");
        var run2w = $(this).attr("run2");
        if (checkThreshold(run1w, run1Threshold) && checkThreshold(run2w, run2Threshold)) {
          $(this).addClass("nobackground");
        } else if (checkThreshold(run1w, run1Threshold)) {
          $(this).css("background", "rgba(" + COLOR_B + "," + run2w + ")");
        } else if (checkThreshold(run2w, run2Threshold)) {
          $(this).css("background", "rgba(" + COLOR_A + "," + run1w + ")");
        } else {
          $(this).css("background", 'linear-gradient(rgba('+ COLOR_A + "," + run1w + '),  rgba( '+ COLOR_B + "," + run2w + '))');
        }
      })
    }

    function onCardEnter() {
      var did = $(this).attr('data-docid');
      $('.card[data-docid="' + did + '"').addClass('highlight');
    }

    function onCardLeave() {
      var did = $(this).attr('data-docid');
      $('.card.highlight').removeClass('highlight');
    }

    function onDocInfoClick() {
      var docid = $(this).closest('[data-docid]').attr('data-docid');
      var doc = data.docs[docid];
      $('<div id="DocumentOverlay"></div>').appendTo(document.body)
      var page = $('<div id="DocumentDetails" class="sticky-top"></div>')
        .append($('<div class="close-overlay">X</div>').click(closeDoc))
        .appendTo(document.body);
      var legendTable = $('<table class="fields"></table>')
        .appendTo(page);
      var run1Rank = $(this).closest('[run1-rank]').attr('run1-rank');
      legendTable.append($('<tr></tr>')
        .append($('<th></th>').text(data.meta.run1_name))
        .append($('<td></td>')
          .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_A + ')'))
        )
        .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run1Rank)))
        .append($('<td></td>').append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdA"></div></form>').attr("title", "slide to change weight threshold")))
        .append($('<td><span id="run1Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
      );
      if (!singleRunView) {
        var run2Rank = $(this).closest('[run2-rank]').attr('run2-rank');
        legendTable.append($('<tr></tr>')
          .append($('<th></th>').text(data.meta.run2_name))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_B + ')'))
          )
          .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run2Rank)))
          .append($('<td></td>')
            .append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdB"></div></form>').attr("title", "slide to change weight threshold"))
          ).append($('<td><span id="run2Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
        );
        legendTable.append($('<tr></tr>')
          .append($('<th>both</th>'))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background', 'linear-gradient(rgb(' + COLOR_A + '), rgb(' + COLOR_B + '))'))
          ));
      }
      var fieldTable = $('<table class="fields"></table>')
        .appendTo(page);
      var weightsA = allWeightsA[docid] || {};
      var weightsB = allWeightsB[docid] || {};
      var mweights = mergedWeights[docid] || {};

      $.each(doc, function (fname, fvalue) {
        if (singleRunView) {
          if (!(fname in weightsA)) {
            mweights[fname] = [];
          } else {
            mweights[fname] = weightsA[fname].map(segment => {
              return [segment[0], segment[1], { "run1": segment[2] }];
            });
          }
        }
        var weights = colorizeWeights(mweights[fname]);
        $('<tr></tr>')
          .append($('<th></th>').text(fname))
          .append($('<td></td>').append(markup(fvalue, weights)))
          .appendTo(fieldTable);
      });

      $("input").change(function () {
        var threshold = $(this).closest("form :input").val();
        if ($(this).attr("id") === "weightThresholdA") {
          $("#run1Threshold").text(threshold);
        } else {
          $("#run2Threshold").text(threshold);
        }
        onChangeWeightThreshold();
      });
      onChangeWeightThreshold();
      return false; // prevent nav
    }

    function closeDoc() {
      $('#DocumentOverlay,#DocumentDetails').remove();
    }
    function ding() {
      console.log("Reaching limits! Alert");
    }

    $(function () {
      if (singleRunView) {
        $("#runName").empty();
        $("#runName").append($('<h6 style="text-align: center;"></h6>').text(data.meta.run1_name));
      } else {
        $('#Run1Name').text(data.meta.run1_name);
        $('#Run2Name').text(data.meta.run2_name);
      }
      var $select = $('#Queries');
      var queryDisplayField = null;
      $.each(data.meta.queryFields, function (i, e) {
        if (e !== 'query_id') {
          queryDisplayField = e;
          return false; // break
        }
      });
      $.each(data.queries, function (_, query) {
        if (!singleRunView){
          $('<option>').attr('value', query['fields']['query_id']).attr("data-tokens", query.fields.query_id + " " + query.fields[queryDisplayField]).attr('data-subtext', query.fields.contrast.name+': '+query.fields.contrast.value.toFixed(3)).text(query.fields[queryDisplayField]).appendTo($select);
        } else {
          $('<option>').attr('value', query['fields']['query_id']).text(query.fields[queryDisplayField]).appendTo($select);
        }
      });
      $select.change(selectQuery).change();
      $(document).on('mouseenter', '.card', onCardEnter);
      $(document).on('mouseleave', '.card', onCardLeave);
      $(document).on('click', '.card', onDocInfoClick);
      $(document).on('click', '#DocumentOverlay', closeDoc);
      $(document).keyup(function (e) {
        if (e.key === "Escape") {
          closeDoc();
        }
        if (e.key === "ArrowLeft" && !$("#Queries").is(":focus")) {
          var prev_val = $("#Queries option:selected").prev().val();
          if (typeof prev_val != "undefined") {
            $select.val(prev_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
        if (e.key === "ArrowRight" && !$("#Queries").is(":focus")) {
          var next_val = $("#Queries option:selected").next().val();
          if (typeof next_val != "undefined") {
            $select.val(next_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
      });
    });
    $(document).ready(function () {
      var $select = $('#Queries');
      if (data.queries.length > 20)
        $select.attr("data-live-search","true")
      $select.selectpicker();
      $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse")
      $(".query_collapse").on("shown.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-contract" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M.172 15.828a.5.5 0 0 0 .707 0l4.096-4.096V14.5a.5.5 0 1 0 1 0v-3.975a.5.5 0 0 0-.5-.5H1.5a.5.5 0 0 0 0 1h2.768L.172 15.121a.5.5 0 0 0 0 .707zM15.828.172a.5.5 0 0 0-.707 0l-4.096 4.096V1.5a.5.5 0 1 0-1 0v3.975a.5.5 0 0 0 .5.5H14.5a.5.5 0 0 0 0-1h-2.768L15.828.879a.5.5 0 0 0 0-.707z"/></svg>';
        $("#query-collapse-btn").html(text);
      })
      $(".query_collapse").on("hidden.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-expand" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z"></svg>';
        $("#query-collapse-btn").html(text);
      })
    })
  </script>
</body>

</html>

